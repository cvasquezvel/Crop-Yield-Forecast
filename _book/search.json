[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Crop Yield Forecast in tidymodels",
    "section": "",
    "text": "Bienvenido a Crop yield forecast con tidymodels, una guía que presenta un abanico de técnicas para el modelamiento predictivo de cultivos empleando el entorno de tidymodels (de Julia Silge y Max Kuhn) en el lenguaje de programación R, escrito con los siguientes objetivos:\n\nEn primer lugar, este libro brinda ideas introductorias que sirven para un mayor entendimiento del negocio agrícola, de la problemática en las proyecciones agrícolas de producción y de las necesidades metodológicas que implica estos casos de estudio. Con esto se busca que el lector aumente su capacidad de solucionar problemas reales en la agricultura usando aprendizaje estadístico para la toma de decisiones.\nEn segundo lugar, este libro le mostrará como implementar modelos predictivos en el entorno tidy, considerando distintos criterios para solucionar un problema y dándole alcances sobre como usar cada técnica estadística para obtener resultados exitosos con el software R, evitando errores comunes.\n\nEste libro revisa muy superficialmente la teoría de los métodos estadísticos, y no brinda alcances sobre técnicas de minería de datos y procesos de descubrimiento del conocimiento a partir de datos, por lo que no es recomendable su lectura para aquellos usuarios con interés en formarse en técnicas de aprendizaje estadístico.\nHe escrito esta guía luego de casi dos años de experiencia en modelamiento predictivo de cultivos, enfocándome casi todo ese tiempo en desarrollar algoritmos para la estimación de la productividad en arándanos y donde paralelamente estuve cursando mis estudios de maestría en Estadística Aplicada en la Universidad Nacional Agraria La Molina, Lima, que para julio de 2022 fueron concluidos. Adicionalmente, soy bachiller en Agronomía de la Universidad Nacional Pedro Ruiz Gallo de Lambayeque, Perú, y vengo desarrollándome como consultor externo en análisis estadístico de investigaciones agrícolas desde hace cuatro años."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introducción",
    "section": "",
    "text": "Los productores agrícolas estiman su producción de la campaña y la producción semanal para planificar la compra de materiales, contrato de mano de obra, de transporte, habilitación de almacenes, entre otros, con el objetivo de hacer más rentable la actividad agrícola. Para estos productores, es fundamental garantizar contratos de venta del producto final con aquellos clientes que puedan pagar más; para lograr esto, se necesita estimaciones certeras de la producción, que les permitan negociar y tomar decisiones maniobrables a mediano plazo.\nPor lo general, las estimaciones de producción global de la campaña son certeras. El negocio agrícola considera certera una proyección que tiene de 5 % a 15 % de error predictivo (según el caso). El problema principal está en determinar la distribución de la producción en el tiempo (ya sea a nivel diario, semanal o mensual).\nEn la literatura abundan las técnicas que permiten las estimaciones de cosecha o producción a nivel de campaña, aunque, existe una brecha literaria, muy grande, por cubrir en el caso de estimaciones a nivel semanal.\nPor otro lado, la escasez de profesionales especializados en pronósticos de cosecha y producción agrícola, hace que el problema sea mucho más fuerte. Sobre esto, mi percepción personal es que el talento existe, pero existen factores motivacionales que los orientan a otros rubros. La motivación extrínseca es mucho más fuerte en otros sectores, donde estadísticos altamente capacitados pueden sacar una mayor ventaja económica de sus habilidades (podemos hablar sobre esto en otro momento). Igualmente, podemos hablar de las estrategias empleadas por áreas en formación como algo que se está mejorando con el tiempo, pero, se necesita más empuje e inversión por parte de las empresas agrícolas. El entrenamiento y capacitación es clave cuando se usa profesionales junior. La cabeza del proyecto o área necesariamente debe ser alguien experimentado en modelos predictivos.\nFrente a este problema, han aparecido muchas empresas que brindan servicios de pronósticos de producción. Entre las ventajas de estas empresas están su alto crecimiento e inversión en contratación de profesionales altamente especializados en forecasting, pero, su principal desventaja es que muchas de ellas operan de forma remota, entonces, viven las proyecciones agrícolas solo como una revisión de datos y predicciones sin interpretación por expertos, y no como un arte que necesita de la cooperación con especialistas agrícolas que te ayudan a validar los modelos desde un aspecto más empírico y lógico (pero extremadamente necesario).\nPara hacer frente a estos problemas, es necesario contar con la cooperación académica. Los avances locales sobre estos temas aún son básicos. Podemos ver mayores avances sobre la materia en escuelas profesionales extranjeras. Revisar los artículos científicos que se publican a nivel internacional ayuda a entender el estado del arte de las proyecciones agrícolas y a orientar el modelamiento predictivo hacia las nuevas corrientes que vienen apareciendo en agricultura. No podemos descartar los avances que se tienen en problemas similares (por ejemplo, en predicción clínica)."
  },
  {
    "objectID": "intro.html#crop-yield-forecast",
    "href": "intro.html#crop-yield-forecast",
    "title": "1  Introducción",
    "section": "1.1 Crop Yield Forecast",
    "text": "1.1 Crop Yield Forecast\nEl pronóstico del rendimiento de cultivos es un arte, que nace como parte del modelamiento de cultivos. Las primeras publicaciones en Crop Modelling llegaron de la mano de Duncan y deWit en la década de 1960. Inicialmente se resolvía problemas que se trataban de resolver mediante inferencia, por ejemplo, evaluar los efectos de los niveles de fertilización en los rendimientos y el comportamiento agronómico de cultivos.\nUna de mis historias favoritas de profesionales que dieron los primeros pasos en Crop Modelling, es la de Montgomery, autor del libro Diseño y análisis de experimentos, un estadístico que compró un viñedo y llegó a producir un vino que fue premiado por su alta calidad, como resultado de años de investigación en fertilización, poscosecha de la vid y proceso de producción del vino, respaldado por un alto nivel de análisis estadístico. Por estos tiempos, la inferencia en modelamiento de cultivos es un área ampliamente estudiada y conocida.\nCon el nacimiento del aprendizaje estadístico por parte de Efron y Tibshirani (1991, 1994), se fundan las bases del modelamiento predictivo y es a partir del 2000, con los avances en estadística computacional, que se disparan las investigaciones en diversas ciencias, sobre cómo usar la estadística para resolver problemas reales en predicción. Ahora, es cada vez más accesible tener recursos computacionales (tanto en hardware como software), necesarios para hacer modelamiento predictivo, y esto es un detonante de que cada vez más profesionales exploren sobre este arte.\nEn el caso del Crop Yield Forecast, todos estos avances han servido para resolver, como un problema de regresión o serie de tiempo, diversas estimaciones empleando técnicas de aprendizaje estadístico, estadística espacial, remote sensing, entre otros. Aunque usar imágenes satelitales y multiespectrales para hacer proyecciones de la producción de cultivos es algo muy usual actualmente, sigue siendo difícil encontrar aplicaciones en cultivos de exportación y no tradicionales, sobretodo si se trata de cultivos donde la aparición de flores es continua e “infinita” y solo sirven para estimar producción total de una campaña.\nDebemos entender que de forma empírica, las estimaciones de cosecha se realizan en muchos cultivos teniendo en cuenta una fórmula matemática como la siguiente:\n\\[Producción = Área * Densidad~de~siembra * Número~de~órganos~por~planta * Peso~de~ órganos\\]\nEntendiéndose con esta fórmula que para estimar la producción por campaña de cualquier cultivo, se necesita información de la extensión de siembra, número de plantas por hectárea, número de órganos a cosechar y el peso promedio de estos órganos.\nPara fines logísticos, hacer una estimación de producción a nivel de campaña no ayuda mucho, ya que se necesita saber la producción en distintos puntos de corte o periodos de tiempo.\nPara esto último fin, podemos hacer una modificación a la ecuación, obteniendo lo siguiente:\n\\[Producción_{(t)} = Área * Densidad~de~siembra * Número~de~órganos~por~planta_{(t)} * Peso~de~órganos_{(t)} * Tasa~de~maduración_{(t)}\\]\nEsta última ecuación añade un modelamiento temporal, que está en función de la tasa de maduración y crecimiento de los órganos a cosechar.\nEntonces, para empezar a realizar Crop Yield Forecast, debemos entender que existe en el tiempo uno o más factores que condicionarán la producción en distintos puntos transversales y por ello debemos tomar datos horizontales o de series de tiempo en el cultivo, pues serán fundamentales para realizar las proyecciones. Es así, que debemos imaginar cualquier algoritmo como una función similar a esta última ecuación matemática y debemos lograr que el modelo por crear, a través del aprendizaje estadístico, cumpla con la lógica necesaria para simular este comportamiento matemático (que fue pensado para simular el comportamiento real del cultivo).\nEvidentemente, existen variables como el clima, el riego (agua), el suelo y el hombre (manejo agronómico), que intervienen en la respuesta real de la producción agrícola. Entender la relación que existe entre estas variables y la producción, es fundamental para tener éxito en el modelamiento.\nYa para empezar con tidymodels, mucho más allá del modelamiento predictivo, que es la última fase del proceso del Crop Yield Forecast, el obtener datos calidad es el mayor desafío. Un esquema de muestreo adecuado, es el pilar del éxito. Tener un equipo que sepa como implementar y mejorar un esquema de muestreo es lo más importante, más allá de si se usa mano de obra o inteligencia artificial para recolectar los datos."
  },
  {
    "objectID": "intro.html#tidymodels",
    "href": "intro.html#tidymodels",
    "title": "1  Introducción",
    "section": "1.2 Tidymodels",
    "text": "1.2 Tidymodels\nR es un lenguaje de programación de acceso libre para la comunidad. Es un lenguaje amigable para estadística y ciencia de datos, que es extensible por medio de paquetes o bibliotecas. Tidyverse (Wickham et al., 2019) es una colección de paquetes, entre los más usados en R para ciencia de datos. Tidymodels (Silge & Kuhn, 2020), nace como una extensión para el modelamiento predictivo dentro de tidyverse."
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "2  Modelamiento en tidymodels con el motor lm.",
    "section": "",
    "text": "Caso: Plant spatial traits, bee species composition, and weather conditions dataset for wild blueberry yield prediction through computer simulation modeling and machine learning algorithms (Obsie & Drummond, 2020)"
  },
  {
    "objectID": "chapter1.html#resumen",
    "href": "chapter1.html#resumen",
    "title": "2  Modelamiento en tidymodels con el motor lm.",
    "section": "2.1 Resumen",
    "text": "2.1 Resumen\nEl artículo “Wild blueberry yield prediction using a combination of computer simulation and machine learning algorithms”, usa datos recolectados por 30 años en Maine (EEUU) y publicados por Wild Blueberry Pollination Model, para un modelo de simulación espacial. El modelamiento predictivo en arándanos requiere de datos con fuerte influyencia de factores espaciales (geográficos), y adicionalmente en este caso, de las plantas, especies de abejas y meteorología. Estos datos simulados son usados por investigadores para probar la calidad predictiva de sus algoritmos de aprendizaje estadístico a datos reales y datos generados por simulación. Emplearemos este conjunto de datos para entrenar modelos en el enfoque tidy."
  },
  {
    "objectID": "chapter1.html#descripción-de-los-datos",
    "href": "chapter1.html#descripción-de-los-datos",
    "title": "2  Modelamiento en tidymodels con el motor lm.",
    "section": "2.2 Descripción de los datos",
    "text": "2.2 Descripción de los datos\nEn este artículo se cuenta con un conjunto de 777 registros. En la Tabla 1 se muestra una descripción de las variables empleadas.\nTabla 1.\nDescripción de las variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariables\n\nUnidad\n\nDescripción\n\n\n\n\n\n\n\n\n\n\n\n\n\nClonesize\n\n\\(m^2\\)\n\nEl tamaño medio de los clones de arándanos en el campo\n\n\n\n\n\n\n\n\n\n\n\n\n\nHoneybee\n\nbees/\\(m^2\\)/min\n\nDensidad de abejas melíferas en el campo\n\n\n\n\n\n\n\n\n\n\n\n\n\nBumbles\n\nbees/\\(m^2\\)/min\n\nDensidad de abejorros en el campo\n\n\n\n\n\n\n\n\n\n\n\n\n\nAndrena\n\nbees/\\(m^2\\)/min\n\nDensidad de abejas Andrena en el campo\n\n\n\n\n\n\n\n\n\n\n\n\n\nOsmia\n\nbees/\\(m^2\\)/min\n\nDensidad de abejas Osmia en el campo\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaxOfUpperTRange\n\n℃\n\nEl registro más alto de la temperatura diaria del aire en la banda superior durante la temporada de floración\n\n\n\n\n\n\n\n\n\n\n\n\n\nMinOfUpperTRange\n\n℃\n\nEl registro más bajo de la banda superior de la temperatura diaria del aire temperatura\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverageOfUpperTRange\n\n℃\n\nLa media de la temperatura diaria del aire en la banda superior\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaxOfLowerTRange\n\n℃\n\nEl registro más alto de la temperatura diaria del aire en la banda inferior\n\n\n\n\n\n\n\n\n\n\n\n\n\nMinOfLowerTRange\n\n℃\n\nEl registro más bajo de la banda inferior de la temperatura diaria del aire temperatura\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverageOfLowerTRange\n\n℃\n\nLa media de la banda inferior de la temperatura diaria del aire diaria\n\n\n\n\n\n\n\n\n\n\n\n\n\nRainingDays\n\nDía\n\nEl número total de días durante la estación de floración, cada uno de los cuales tiene una precipitación superior a cero\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverageRainingDays\n\nDía\n\nLa media de días de lluvia de toda la floración temporada\n\n\n\n\n\n\n\n\n\n\n\n\n\nFruitset\n\n100 unidades\n\nNúmero de frutos por planta en centenas\n\n\n\n\n\n\n\n\n\n\n\n\n\nFruitmass\n\nkg\n\nPeso de frutos\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeeds\n\nUnidad\n\nNúmero de semillas por baya\n\n\n\n\n\n\n\n\n\n\n\n\n\nYield\n\nkg\n\nRendimiento\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn este problema de estudio se afirma que existe independencia entre observaciones, a pesar que recolecta datos de clima durante el periodo 2015 a 2019. Pero, analizando más a fondo, pareciera que existe dependencia de individuos o puntos de muestreo (posiblemente, habrían entre clonesize datos de individuos con medidas repetidas en el tiempo) y cada cada uno de estos puntos de muestreo está en un punto único en el espacio. Por lo tanto, en este caso, existiría dependencia espacio - temporal, con datos de individuos con medidas repetidas. Omitiremos los supuestos de este caso para emplear modelos básicos y ver como nos va.\nDe las 777 observaciones, tomaremos 354 (45.56 %) , que corresponden a todas aquellas observaciones con clonesize mayor o igual a 20, para realizar un testeo externo de los modelos. Con las 407 observaciones restantes realizaremos un entrenamiento, validación cruzada y testeo interno de los modelos.\n\ndata %>% summarytools::descr()\n\nDescriptive Statistics  \n\n                    andrena   AverageOfLowerTRange   AverageOfUpperTRange   AverageRainingDays\n----------------- --------- ---------------------- ---------------------- --------------------\n             Mean      0.45                  48.55                  68.64                 0.32\n          Std.Dev      0.16                   5.40                   7.65                 0.17\n              Min      0.00                  41.20                  58.20                 0.06\n               Q1      0.38                  45.80                  64.70                 0.10\n           Median      0.38                  50.80                  71.90                 0.26\n               Q3      0.50                  50.80                  71.90                 0.39\n              Max      0.75                  55.90                  79.00                 0.56\n              MAD      0.18                   7.41                  10.53                 0.24\n              IQR      0.12                   5.00                   7.20                 0.29\n               CV      0.36                   0.11                   0.11                 0.54\n         Skewness      0.23                   0.00                  -0.01                 0.07\n      SE.Skewness      0.12                   0.12                   0.12                 0.12\n         Kurtosis     -0.50                  -1.33                  -1.34                -1.29\n          N.Valid    423.00                 423.00                 423.00               423.00\n        Pct.Valid    100.00                 100.00                 100.00               100.00\n\nTable: Table continues below\n\n \n\n                    bumbles   clonesize   fruitmass   fruitset   honeybee   MaxOfLowerTRange\n----------------- --------- ----------- ----------- ---------- ---------- ------------------\n             Mean      0.28       12.78        0.46       0.54       0.36              59.26\n          Std.Dev      0.07        1.44        0.04       0.07       1.32               6.63\n              Min      0.00       10.00        0.31       0.19       0.00              50.20\n               Q1      0.25       12.50        0.43       0.50       0.25              55.80\n           Median      0.25       12.50        0.46       0.55       0.25              62.00\n               Q3      0.25       12.50        0.49       0.59       0.25              62.00\n              Max      0.58       20.00        0.54       0.65      18.43              68.20\n              MAD      0.00        0.00        0.05       0.07       0.00               9.19\n              IQR      0.00        0.00        0.06       0.10       0.00               6.20\n               CV      0.25        0.11        0.09       0.13       3.62               0.11\n         Skewness     -0.10        4.76       -0.44      -0.99      12.56              -0.02\n      SE.Skewness      0.12        0.12        0.12       0.12       0.12               0.12\n         Kurtosis      2.76       21.05       -0.15       1.29     164.04              -1.35\n          N.Valid    423.00      423.00      423.00     423.00     423.00             423.00\n        Pct.Valid    100.00      100.00      100.00     100.00     100.00             100.00\n\nTable: Table continues below\n\n \n\n                    MaxOfUpperTRange   MinOfLowerTRange   MinOfUpperTRange    osmia   RainingDays\n----------------- ------------------ ------------------ ------------------ -------- -------------\n             Mean              82.20              28.66              49.62     0.56         18.20\n          Std.Dev               9.16               3.20               5.60     0.18         12.17\n              Min              69.70              24.30              39.00     0.00          1.00\n               Q1              77.40              27.00              46.80     0.50          3.77\n           Median              86.00              30.00              52.00     0.63         16.00\n               Q3              86.00              30.00              52.00     0.75         24.00\n              Max              94.60              33.00              57.20     0.75         34.00\n              MAD              12.75               4.45               7.71     0.18         18.13\n              IQR               8.60               3.00               5.20     0.25         20.23\n               CV               0.11               0.11               0.11     0.32          0.67\n         Skewness              -0.02              -0.01              -0.02    -1.01         -0.19\n      SE.Skewness               0.12               0.12               0.12     0.12          0.12\n         Kurtosis              -1.34              -1.34              -1.33     0.75         -1.28\n          N.Valid             423.00             423.00             423.00   423.00        423.00\n        Pct.Valid             100.00             100.00             100.00   100.00        100.00\n\nTable: Table continues below\n\n \n\n                     seeds     yield\n----------------- -------- ---------\n             Mean    37.84   6559.03\n          Std.Dev     4.37   1273.98\n              Min    22.08   1637.70\n               Q1    35.09   5686.12\n           Median    38.38   6779.98\n               Q3    41.25   7529.18\n              Max    46.59   8969.40\n              MAD     4.55   1255.22\n              IQR     6.13   1826.46\n               CV     0.12      0.19\n         Skewness    -0.45     -0.64\n      SE.Skewness     0.12      0.12\n         Kurtosis    -0.09      0.02\n          N.Valid   423.00    423.00\n        Pct.Valid   100.00    100.00"
  },
  {
    "objectID": "chapter1.html#partición-inicial-de-datos",
    "href": "chapter1.html#partición-inicial-de-datos",
    "title": "2  Modelamiento en tidymodels con el motor lm.",
    "section": "2.3 Partición inicial de datos",
    "text": "2.3 Partición inicial de datos\nA las últimas 423 observaciones no filtradas (clonesize < 25), se les aplicó una partición inicial 70 / 20, para obtener 296 observaciones para validación cruzada y entrenamiento de un modelo final, y 127 observaciones para testeo interno (en adición, recordemos que en un filtrado inicial se separó 354 observaciones para un testeo externo).\n\n### Data partition ----\n\nset.seed(123)\ndata_split <- initial_split(data, prop = 0.7)\ndata_train <- training(data_split)\ndata_test <- testing(data_split)"
  },
  {
    "objectID": "chapter1.html#receta-de-preprocesamiento",
    "href": "chapter1.html#receta-de-preprocesamiento",
    "title": "2  Modelamiento en tidymodels con el motor lm.",
    "section": "2.4 Receta de preprocesamiento",
    "text": "2.4 Receta de preprocesamiento\nCon ayuda del paquete {recipes}, se pueden crear recetas de preprocesamiento mediante pasos que ayudan en los procesos de ingeniería de variables. En este ejercicio, decido transformar con la función logaritmo natural a todas las variables numéricas, excepto a todas aquellas cuyo valor mínimo sea menor a 1.\nEste paquete permite aplicar la receta creada a cualquier conjunto de datos. Primero se tiene que hacer una preparación de la receta con la función prep(), y la transformación de los datos se emplea con juice() y bake(), para los datos con los que se realiza la base del proceso y nuevos datos respectivamente.\nEsta receta creada será llamada durante todo el proceso de modelamiento. Por otro lado, cuando una receta tiene demasiados pasos, es recomendable transformar los conjuntos de datos con la función bake() y luego crear una receta genérica con la cual hacer el modelamiento.\n\n### Recipes ----\n\ndata_rec <- recipe(formula = `yield` ~ .,\n                   x = data_train) %>%\n  step_select(where(base::is.numeric)) %>%\n  # step_nzv(all_predictors(),\n  #          -all_outcomes()) %>%\n  # step_zv(all_predictors(),\n  #          -all_outcomes()) %>%\n  # step_mutate(honeybee = honeybee * 100,\n  #             bumbles = bumbles * 100,\n  #             andrena = andrena * 100,\n  #             osmia = osmia * 100,\n  #             andrenaosmia = andrena + osmia,\n  #             fruitset = fruitset * 10000,\n  #             fruitmass = fruitmass * 10000) %>%\n  step_mutate_at(where(base::is.numeric),\n                 -contains(c(\"RainingDays\",\n                             \"AverageRainingDays\",\n                             \"honeybee\",\n                             \"bumbles\",\n                             \"andrena\",\n                             \"osmia\",\n                             \"fruitset\",\n                             \"fruitmass\")),\n                 fn = ~log(.)) #%>%\n  # step_corr(all_predictors(),\n  #           -all_outcomes(),\n  #           threshold = 0.9)\n\ndata_prep <- prep(data_rec)\ndata_train_prep <- juice(data_prep)\n# data_train_prep <- bake(data_prep, new_data = data_train)\ndata_test_prep  <- bake(data_prep, new_data = data_test)\ndata_new_prep  <- bake(data_prep, new_data = data_new)"
  },
  {
    "objectID": "chapter1.html#selección-de-variables",
    "href": "chapter1.html#selección-de-variables",
    "title": "2  Modelamiento en tidymodels con el motor lm.",
    "section": "2.5 Selección de variables",
    "text": "2.5 Selección de variables\nUno de los pasos más importantes en el modelamiento, es elegir las variables adecuadas. En esta sección emplearemos un proceso de bajo costo computacional.\nAlgo que se debe tener en cuenta, es que en las proyecciones agrícolas se deben emplear procesos rápidos. No les muestro un proceso que puede tomar mucho tiempo computacional, debido a que no les ayudará de mucho tomarse más tiempo por un resultado de selección que puede llegar a ser muy similar a este.\n\n2.5.1 Selección por Regresión Lasso\n\n2.5.1.1 Proceso paralelo\n\n### Proceso paralelo ----\n\n# Acelerar el cálculo con procesamiento paralelo\n\ndoParallel::registerDoParallel()\n\nncores <- parallel::detectCores(logical = TRUE)\nregisterDoParallel(cores = ncores)\n\n\n\n2.5.1.2 Definir validación cruzada\nPara realizar la validación cruzada, se tomará en cuenta un proceso que es básico y muy conocido, llamado K-fold CV. En este caso, se creará 20 folios. Debemos tener en cuenta que este modelo de validación se emplea cuando existe total independencia entre observaciones.\n\nset.seed(234)\ndata_folds <- rsample::vfold_cv(data_train,\n                                 v = 20)\n\n\n\n2.5.1.3 Definir especificaciones del modelo\nPara crear modelos en tidymodels, es necesario especificar el modelo a emplear, los parámetros del modelo, qué parámetros se calibrarán (tune), el modo o problema del modelo (regresión) y el modelo de motor.\n\nlasso_spec <- linear_reg(penalty = tune(),\n                         mixture = 1) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"glmnet\",\n             standarize = TRUE)\n\n\n\n2.5.1.4 Definir fórmula del modelo\nEn este paso, definimos que la formula del rendimiento incluirá a todas las variables del dataset.\n\nformula_lasso = yield ~ .\n\n\n\n2.5.1.5 Definir flujo de trabajo\nEl flujo de trabajo contiene la información requerida para ajustar y predecir desde un modelo.\n\nlasso_wf <- workflow() %>%\n  add_recipe(data_rec) %>%\n  add_model(lasso_spec,\n            formula = formula_lasso) \n\n\n\n2.5.1.6 Extraer el conjunto de parámetros del modelo\nEn este paso, se extraen los parámetros a calibrar, que fueron definidos en las especificaciones del modelo.\n\nlasso_params <- hardhat::extract_parameter_set_dials(lasso_wf)\nlasso_params\n\nCollection of 1 parameters for tuning\n\n identifier    type    object\n    penalty penalty nparam[+]\n\n\n\n\n2.5.1.7 Definir métricas del performance predictivo\nEn este paso elegimos qué metricas vamos a rescatar en las distintas fases de evaluación del modelo.\n\nmulti_met <- yardstick::metric_set(yardstick::rmse, yardstick::rsq,\n                                   yardstick::mape, yardstick::mae)\n\n\n\n2.5.1.8 Definir función para recuperar coeficientes\nDefinimos una función que nos permita rescatar los coeficientes del modelo.\n\nget_glm_coefs <- function(x) {\n  x %>% \n    # get the glm model object\n    extract_fit_engine() %>% \n    # transform its format\n    broom::tidy()\n}\n\n\n\n2.5.1.9 Búsqueda del mejor índice de penalización\nEn este paso, ejecutamos la validación cruzada y realizamos la búsqueda del mejor lambda.\n\nset.seed(2020)\ntictoc::tic()\nlasso_tune <-\n  lasso_wf %>%\n  tune_grid(\n    resamples = data_folds,\n    param_info = lasso_params,\n    grid = 100, #lambda_grid\n    # por defecto es 5 (número aleatorio de combinaciones (puntos de grilla) de hiperparámetros)\n    metrics = multi_met, \n    control = control_grid(\n      extract = get_glm_coefs,\n      save_pred = TRUE,\n      verbose = FALSE)\n    )\ntictoc::toc()\n\n17.22 sec elapsed\n\n\n\n\n2.5.1.10 Coeficientes del modelo en cada folio\nEn este paso, podemos observar la evolución de los coeficientes de las variables para cada folio. Adicional, tenemos un autoplot() que grafica la evolución de las métricas a medida que cambia el lambda.\n\nlasso_coefs <-\n  lasso_tune %>%\n  select(id, .extracts) %>%\n  unnest(.extracts) %>%\n  # La columna `penalty` en este nivel es redundante, así que\n  # la eliminaremos.\n  select(id, .extracts) %>%\n  # Como se obtienen todos los coeficientes para cada ajuste glmnet, los valores \n  # se replican dentro de un valor de mezcla. Mantendremos la primera fila para \n  # no obtener los mismos valores una y otra vez.\n  group_by(id) %>%\n  dplyr::slice(1) %>%\n  ungroup() %>%\n  unnest(.extracts) %>%\n  # cambiar el nombre para que sea coherente con tidymodels\n  rename(\"penalty\" = \"lambda\")\n\nlasso_coefs %>% select(id, term, penalty, estimate)\n\n# A tibble: 15,608 × 4\n   id     term        penalty estimate\n   <chr>  <chr>         <dbl>    <dbl>\n 1 Fold01 (Intercept)  0.224      8.76\n 2 Fold01 (Intercept)  0.204      8.62\n 3 Fold01 (Intercept)  0.186      8.49\n 4 Fold01 (Intercept)  0.169      8.37\n 5 Fold01 (Intercept)  0.154      8.26\n 6 Fold01 (Intercept)  0.141      8.16\n 7 Fold01 (Intercept)  0.128      8.07\n 8 Fold01 (Intercept)  0.117      7.98\n 9 Fold01 (Intercept)  0.106      7.91\n10 Fold01 (Intercept)  0.0970     7.84\n# … with 15,598 more rows\n\nlasso_coefs %>%\n  filter(term != \"(Intercept)\") %>%\n  ggplot(aes(x = term, y = estimate, group = id, col = id)) +\n  geom_hline(yintercept = 0, lty = 3) +\n  geom_line(alpha = 0.3, linewidth = 1.2) +\n  labs(y = \"Coefficient\", x = NULL) +\n  theme(legend.position = \"none\") +\n  theme(axis.text.x=element_text(angle=90, hjust=0.5))\n\n\n\nautoplot(lasso_tune)\n\n\n\n\n\n\n2.5.1.11 Seleccionar el mejor valor de penalización\nAquí, buscamos el lambda con menor rmse y con este valor entrenaremos el modelo final.\n\nbest_metrics <- lasso_tune %>%\n  select_best(\"rmse\")\nbest_metrics\n\n# A tibble: 1 × 2\n   penalty .config               \n     <dbl> <chr>                 \n1 0.000102 Preprocessor1_Model061\n\nfinal_lasso <- finalize_workflow(lasso_wf,\n                                 best_metrics)\nfinal_lasso\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_select()\n• step_mutate_at()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0.000102173480970979\n  mixture = 1\n\nEngine-Specific Arguments:\n  standarize = TRUE\n\nComputational engine: glmnet \n\n\n\n\n2.5.1.12 Entrenar el modelo final\n\nfinal <- final_lasso %>%\n  fit(data_train) %>%\n  extract_fit_parsnip()\n\n# final\n\n\n\n2.5.1.13 Predictores finales\nComo último paso, extraemos los predictores finales, usando la importancia de estas variables en el modelo final, omitiendo todos aquellos predictores con importancia 0.\n\npredictors <- final %>%\n  vip::vi(lambda = best_metrics$penalty,\n          alpha = best_metrics$mixture,\n          num_features = 60) %>%\n  dplyr::mutate(\n    Importance = abs(Importance),\n    Variable = fct_reorder(Variable, Importance)\n  ) %>%\n  dplyr::filter(!Importance == 0) %>% \n  dplyr::arrange(-Importance) %>%\n  select(Variable) %>% t() %>% as.vector()\n\npredictors\n\n [1] \"fruitmass\"          \"fruitset\"           \"seeds\"             \n [4] \"osmia\"              \"andrena\"            \"bumbles\"           \n [7] \"MinOfUpperTRange\"   \"clonesize\"          \"AverageRainingDays\"\n[10] \"honeybee\"           \"RainingDays\""
  },
  {
    "objectID": "chapter1.html#modelo-lineal-general",
    "href": "chapter1.html#modelo-lineal-general",
    "title": "2  Modelamiento en tidymodels con el motor lm.",
    "section": "2.6 Modelo lineal general",
    "text": "2.6 Modelo lineal general\nEn esta sección, veremos como se entrenan, validen y testean modelos paramétricos, tomando como ejemplo el modelo lineal general. Omitiré la explicación de los pasos que vimos en la sección anterior.\n\n2.6.1 Definir especificaciones del modelo\n\n# Inicializar un objeto de regresión lineal\nlinear_model_spec <- linear_reg() %>% \n  # Establecer el modelo de motor\n  set_engine('lm') %>% \n  # Establecer el modo de modelo\n  set_mode('regression')\n\n\n\n2.6.2 Definir fórmula del modelo\nA diferencia de la sección anterior, ahora definimos la fórmula con los predictores elegidos por la regresión Lasso.\n\nformula_lm = yield ~ fruitmass + fruitset + seeds + osmia + andrena + bumbles +\n  MinOfUpperTRange + clonesize + AverageRainingDays + honeybee + RainingDays  \n\n\n\n2.6.3 Definir flujo de trabajo\n\nlm_wf <- workflow() %>%\n  add_recipe(data_rec) %>%\n  add_model(linear_model_spec,\n            formula = formula_lm) \n\n\n\n2.6.4 Validación cruzada\n\ntictoc::tic()\nvalidacion_lm <-\n  lm_wf %>%\n  fit_resamples(\n    resamples = data_folds,\n    metrics = multi_met,\n    control = control_resamples( #control_grid\n      extract = get_glm_coefs, #identity\n      save_pred = TRUE,\n      verbose = FALSE#,\n      #parallel_over = \"resamples\"\n    )\n  )\ntictoc::toc()\n\n3.34 sec elapsed\n\n\nLuego de la validación cruzada, como la función considera que yield está transformado a su logaritmo natural, al aplicarle un exponente tanto a los valores reales como los valores predichos, estaremos obteniendo los valores en la escala original. Haremos lo mismo en muchos pasos siguientes.\n\nvalidacion_lm <- validacion_lm %>%\n  unnest(.predictions) %>%\n  dplyr::mutate(yield = exp(yield),\n                .pred = exp(.pred)) %>%\n  group_by(id) %>%\n  nest(`.predictions` = 6:9)\n\n\n\n2.6.5 Coeficientes en cada folio\n\nlm_coefs <-\n  validacion_lm %>%\n  select(id, .extracts) %>%\n  unnest(.extracts) %>%\n  select(id, .extracts) %>%\n  group_by(id) %>%\n  dplyr::slice(1) %>%\n  ungroup() %>%\n  unnest(.extracts)\n\nlm_coefs %>% select(id, term, estimate)\n\n# A tibble: 240 × 3\n   id     term               estimate\n   <chr>  <chr>                 <dbl>\n 1 Fold01 (Intercept)         2.04   \n 2 Fold01 fruitmass          -5.48   \n 3 Fold01 fruitset            2.31   \n 4 Fold01 seeds               2.21   \n 5 Fold01 osmia               0.0408 \n 6 Fold01 andrena             0.0286 \n 7 Fold01 bumbles            -0.0713 \n 8 Fold01 MinOfUpperTRange    0.00523\n 9 Fold01 clonesize          -0.00461\n10 Fold01 AverageRainingDays  0.00593\n# … with 230 more rows\n\nlm_coefs %>%\n  filter(term != \"(Intercept)\") %>%\n  ggplot(aes(x = term, y = estimate, group = id, col = id)) +\n  geom_hline(yintercept = 0, lty = 3) +\n  geom_line(alpha = 0.3, linewidth = 1.2) +\n  labs(y = \"Coefficient\", x = NULL) +\n  theme(legend.position = \"none\") +\n  theme(axis.text.x=element_text(angle=90, hjust=0.5))\n\n\n\n\n\n\n2.6.6 Métricas del rendimiento predictivo en fase de validación cruzada\nEn este paso recupero las métricas del rendimiento predictivo en la fase de validación cruzada, primero como promedio para cada folio y luego el valor puntual obtenido de cada folio.\n\n# Métricas promedio de todas las particiones\nmetrics_lm <- \n  validacion_lm %>%\n  unnest(.predictions) %>%\n  group_by(id) %>%\n  multi_met(truth = `yield`,\n            estimate = `.pred`) %>%\n  group_by(.metric) %>%\n  dplyr::summarise(mean = mean(`.estimate`, na.rm = T),\n            std_err = sd(`.estimate`, na.rm = T),\n            n = n()) %>%\n  ungroup() %>%\n  dplyr::mutate(modelo = \"lm\")\nmetrics_lm\n\n# A tibble: 4 × 5\n  .metric    mean  std_err     n modelo\n  <chr>     <dbl>    <dbl> <int> <chr> \n1 mae      80.7   17.1        20 lm    \n2 mape      1.36   0.509      20 lm    \n3 rmse    105.    29.0        20 lm    \n4 rsq       0.994  0.00244    20 lm    \n\n\n\n# Métricas individuales de cada una de las particiones\nmetrics_lm_complete <- \n  validacion_lm %>%\n  unnest(.predictions) %>%\n  group_by(id) %>%\n  multi_met(truth = `yield`,\n            estimate = `.pred`) %>%\n  ungroup() %>%\n  dplyr::mutate(modelo = \"lm\")\nmetrics_lm_complete\n\n# A tibble: 80 × 5\n   id     .metric .estimator .estimate modelo\n   <chr>  <chr>   <chr>          <dbl> <chr> \n 1 Fold01 rmse    standard       115.  lm    \n 2 Fold02 rmse    standard       193.  lm    \n 3 Fold03 rmse    standard        85.5 lm    \n 4 Fold04 rmse    standard       117.  lm    \n 5 Fold05 rmse    standard       102.  lm    \n 6 Fold06 rmse    standard        70.4 lm    \n 7 Fold07 rmse    standard        92.7 lm    \n 8 Fold08 rmse    standard        92.6 lm    \n 9 Fold09 rmse    standard       106.  lm    \n10 Fold10 rmse    standard       131.  lm    \n# … with 70 more rows\n\n\nLuego, ploteamos los resultados del rendimiento predictivo en fase de validación cruzada.\n\n# Valores de validación (mae y rmse) obtenidos en cada partición y repetición.\np1 <- ggplot(\n  data = metrics_lm_complete,\n  aes(x = .estimate, fill = .metric)) +\n  geom_density(alpha = 0.5) +\n  theme_bw() +\n  facet_wrap(. ~ .metric, scales = \"free\") +\n  geom_vline(\n    data = metrics_lm %>% rename(\".estimate\" = \"mean\"),\n    aes(xintercept = .estimate, colour = .metric), linewidth = 1, alpha = 1,\n    linetype = 2\n  ) +\n  theme(axis.text.x=element_text(angle=90, hjust=0.5))\np2 <- ggplot(\n  data = metrics_lm_complete,\n  aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.1) +\n  geom_jitter(width = 0.05, alpha = 0.3) +\n  # coord_flip() +\n  geom_point(\n    data = metrics_lm %>% rename(\".estimate\" = \"mean\"),\n    color = \"black\", size = 2, alpha = 0.5\n  ) +\n  theme_bw() +\n  facet_wrap(.metric ~ ., scales = \"free\") \n\nggpubr::ggarrange(p1, p2, ncol = 2, common.legend = TRUE, align = \"v\") %>% \n  ggpubr::annotate_figure(\n    top = ggpubr::text_grob(\"Distribución errores de validación cruzada\", size = 15)\n  )\n\n\n\n\n\nvalidation_lm_predictions <- \n  validacion_lm %>% \n  unnest(.predictions) %>%\n  group_by(.row) %>%\n  dplyr::summarise(dplyr::across(c(yield,.pred),\n                                 mean)) %>%\n  dplyr::mutate(modelo = \"lm\")\n\np1 <- ggplot(\n  data = validation_lm_predictions,\n  aes(x = yield, y = .pred)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(slope = 1, intercept = 0, color = \"firebrick\") +\n  labs(title = \"Valor predicho vs valor real\") +\n  theme_bw()\n\np2 <- ggplot(\n  data = validation_lm_predictions,\n  aes(x = .row, y = yield - .pred)\n) +\n  geom_point(alpha = 0.3) +\n  geom_hline(yintercept =  0, color = \"firebrick\") +\n  labs(title = \"Residuos del modelo\") +\n  theme_bw()\n\np3 <- ggplot(\n  data = validation_lm_predictions,\n  aes(x = yield - .pred)\n) +\n  geom_density() + \n  labs(title = \"Distribución residuos del modelo\") +\n  theme_bw()\n\np4 <- ggplot(\n  data = validation_lm_predictions,\n  aes(sample = yield - .pred)\n) +\n  geom_qq() +\n  geom_qq_line(color = \"firebrick\") +\n  labs(title = \"Q-Q residuos del modelo\") +\n  theme_bw()\n\nggpubr::ggarrange(plotlist = list(p1, p2, p3, p4)) %>%\n  ggpubr::annotate_figure(\n    top = ggpubr::text_grob(\"Distribución residuos\", size = 15, face = \"bold\")\n  )\n\n\n\n\n\n\n2.6.7 Entrenamiento del modelo final\nEl entrenamiento de un modelo final se hace con todos los datos de entrenamiento (data_train).\n\nlm_model <- linear_model_spec %>%\n  fit(formula_lm,\n      data = data_train_prep)\nlm_model\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = yield ~ fruitmass + fruitset + seeds + osmia + \n    andrena + bumbles + MinOfUpperTRange + clonesize + AverageRainingDays + \n    honeybee + RainingDays, data = data)\n\nCoefficients:\n       (Intercept)           fruitmass            fruitset               seeds  \n          2.145447           -5.431983            2.352563            2.157387  \n             osmia             andrena             bumbles    MinOfUpperTRange  \n          0.040705            0.028038           -0.066671            0.007771  \n         clonesize  AverageRainingDays            honeybee         RainingDays  \n         -0.002402            0.023102            0.005162           -0.000696  \n\n\n\nlm_model %>% broom::tidy()\n\n# A tibble: 12 × 5\n   term                estimate std.error statistic  p.value\n   <chr>                  <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)         2.15      0.397        5.40  1.42e- 7\n 2 fruitmass          -5.43      0.341      -15.9   2.51e-41\n 3 fruitset            2.35      0.104       22.7   1.04e-65\n 4 seeds               2.16      0.140       15.4   2.30e-39\n 5 osmia               0.0407    0.00902      4.51  9.32e- 6\n 6 andrena             0.0280    0.00787      3.56  4.33e- 4\n 7 bumbles            -0.0667    0.0258      -2.59  1.02e- 2\n 8 MinOfUpperTRange    0.00777   0.0178       0.435 6.64e- 1\n 9 clonesize          -0.00240   0.0191      -0.126 9.00e- 1\n10 AverageRainingDays  0.0231    0.0604       0.383 7.02e- 1\n11 honeybee            0.00516   0.00164      3.15  1.80e- 3\n12 RainingDays        -0.000696  0.000817    -0.852 3.95e- 1\n\n\n\n2.6.7.1 Rescatando las especififaciones del modelo para futuras recalibraciones\n\nspec <- lm_model %>% extract_spec_parsnip()\n\n\n\n\n2.6.8 Testeo interno\nAhora, testeamos el modelo final con los datos de prueba (data_test).\n\n#### Testeo interno ----\n\n# PREDICCIÓN TEST ----\n# =============================================================================\npredicciones <- lm_model %>%\n  predict(\n    new_data = data_test_prep,\n    type = \"numeric\"\n  )\n\n# MÉTRICAS TEST ----\n# =============================================================================\npredicciones_lm <- predicciones %>% \n  bind_cols(data_test_prep %>% dplyr::select(yield)) %>%\n  dplyr::mutate(modelo = \"lm\",\n                yield = exp(yield),\n                .pred = exp(.pred))\n\nsummary(predicciones_lm)\n\n     .pred          yield         modelo         \n Min.   :3154   Min.   :3049   Length:127        \n 1st Qu.:5856   1st Qu.:5792   Class :character  \n Median :6761   Median :6796   Mode  :character  \n Mean   :6625   Mean   :6616                     \n 3rd Qu.:7485   3rd Qu.:7434                     \n Max.   :8651   Max.   :8562                     \n\n# Error de test\n\nerror_test_lm  <- multi_met(\n  data     = predicciones_lm,\n  truth    = yield,\n  estimate = .pred,\n  na_rm    = TRUE\n) %>%\n  dplyr::mutate(\n    modelo = \"lm\"\n  )\nerror_test_lm\n\n# A tibble: 4 × 4\n  .metric .estimator .estimate modelo\n  <chr>   <chr>          <dbl> <chr> \n1 rmse    standard      97.5   lm    \n2 rsq     standard       0.993 lm    \n3 mape    standard       1.25  lm    \n4 mae     standard      79.8   lm    \n\ncombo_plot <- metrics_lm_complete %>% \n  ggplot(aes(x = .metric, y = .estimate)) +\n  geom_jitter(width = 0.2) +\n  geom_boxplot(width = 0.3, alpha = 0.5) +\n  geom_point(\n    data = metrics_lm %>% rename(\".estimate\" = \"mean\"),\n    color = \"green\", size = 2, alpha = 0.5\n  ) +\n  geom_point(\n    data = error_test_lm,\n    color = \"red\", size = 2\n  ) +\n  facet_wrap(.metric~., scales = \"free\")\ncombo_plot\n\n\n\npe1 <- ggplot(\n  data = predicciones_lm,\n  aes(x = yield, y = .pred)\n) +\n  geom_point(alpha = 0.3) +\n  geom_abline(slope = 1, intercept = 0, color = \"firebrick\") +\n  labs(title = \"Valor predicho vs valor real\") +\n  theme_bw()\n\npe2 <- ggplot(\n  data = predicciones_lm,\n  aes(x = 1:nrow(predicciones_lm), y = yield - .pred)\n) +\n  geom_point(alpha = 0.3) +\n  geom_hline(yintercept =  0, color = \"firebrick\") +\n  labs(title = \"Residuos del modelo\",\n       x = \".row\")   +\n  theme_bw()\n\npe3 <- ggplot(\n  data = predicciones_lm,\n  aes(x = yield - .pred)\n) +\n  geom_density() + \n  labs(title = \"Distribución residuos del modelo\") +\n  theme_bw()\n\npe4 <- ggplot(\n  data = predicciones_lm,\n  aes(sample = yield - .pred)\n) +\n  geom_qq() +\n  geom_qq_line(color = \"firebrick\") +\n  labs(title = \"Q-Q residuos del modelo\") +\n  theme_bw()\n\nggpubr::ggarrange(plotlist = list(pe1, pe2, pe3, pe4)) %>%\n  ggpubr::annotate_figure(\n    top = ggpubr::text_grob(\"Distribución residuos\", size = 15, face = \"bold\")\n  )\n\n\n\n\nLos puntos verdes y rojos en los gráficos de cajas y bigotes, representan a los promedios en la fase de validación cruzada y testeo interno, respectivamente.\n\n\n2.6.9 Recalibración inicial del modelo\nComo la prueba fue exitosa y no hay evidencia de sobreajuste, se procede con la recalibración del modelo con los datos de entrenamiento y prueba interna (data_train + data_test).\n\nlm_model_rec <- lm_model %>%\n  extract_spec_parsnip() %>%\n  fit(\n    formula_lm,\n    data = bind_rows(data_train_prep,\n                     data_test_prep))\n\nlm_model_rec %>% broom::tidy()\n\n# A tibble: 12 × 5\n   term                estimate std.error statistic  p.value\n   <chr>                  <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)         2.29      0.325       7.05   7.55e-12\n 2 fruitmass          -5.34      0.266     -20.1    5.02e-63\n 3 fruitset            2.37      0.0934     25.3    5.06e-86\n 4 seeds               2.11      0.115      18.4    1.55e-55\n 5 osmia               0.0378    0.00721     5.24   2.58e- 7\n 6 andrena             0.0349    0.00612     5.69   2.40e- 8\n 7 bumbles            -0.0645    0.0203     -3.18   1.60e- 3\n 8 MinOfUpperTRange   -0.000302  0.0142     -0.0213 9.83e- 1\n 9 clonesize          -0.00463   0.0159     -0.291  7.71e- 1\n10 AverageRainingDays -0.000481  0.0470     -0.0102 9.92e- 1\n11 honeybee            0.00482   0.00149     3.24   1.29e- 3\n12 RainingDays        -0.000489  0.000633   -0.773  4.40e- 1\n\n\n\n\n2.6.10 Testeo externo\nFinalmente, el testeo externo con el conjunto de datos para testeo externo (data_new).\n\n#### Testeo externo ----\n\n# PREDICCIÓN TEST ----\n# =============================================================================\npredicciones_ext <- lm_model_rec %>%\n  predict(\n    new_data = data_new_prep,\n    type = \"numeric\"\n  )\n\n# MÉTRICAS TEST ----\n# =============================================================================\npredicciones_lm_ext <- predicciones_ext %>% \n  bind_cols(data_new_prep %>% select(yield)) %>%\n  dplyr::mutate(modelo = \"lm\",\n                yield = exp(yield),\n                .pred = exp(.pred))\n\nsummary(predicciones_lm_ext)\n\n     .pred          yield         modelo         \n Min.   :2364   Min.   :1946   Length:354        \n 1st Qu.:4452   1st Qu.:4514   Class :character  \n Median :5370   Median :5555   Mode  :character  \n Mean   :5218   Mean   :5360                     \n 3rd Qu.:5979   3rd Qu.:6237                     \n Max.   :7205   Max.   :7805                     \n\n# Error de test\n\nerror_test_lm_ext  <- multi_met(\n  data     = predicciones_lm_ext,\n  truth    = yield,\n  estimate = .pred,\n  na_rm    = TRUE\n) %>%\n  dplyr::mutate(\n    modelo = \"lm\"\n  )\nerror_test_lm_ext\n\n# A tibble: 4 × 4\n  .metric .estimator .estimate modelo\n  <chr>   <chr>          <dbl> <chr> \n1 rmse    standard     240.    lm    \n2 rsq     standard       0.983 lm    \n3 mape    standard       3.56  lm    \n4 mae     standard     193.    lm    \n\ncombo_plot <- metrics_lm_complete %>%\n  ggplot(aes(x = .metric, y = .estimate)) +\n  geom_jitter(width = 0.2) +\n  geom_boxplot(width = 0.3, alpha = 0.5) +\n  geom_point(\n    data = metrics_lm %>% rename(\".estimate\" = \"mean\"),\n    color = \"green\", size = 2, alpha = 0.5\n  ) +\n  geom_point(\n    data = error_test_lm,\n    color = \"red\", size = 2, alpha = 0.5\n  ) +\n  geom_point(\n    data = error_test_lm_ext,\n    color = \"blue\", size = 2, alpha = 0.5\n  ) +\n  facet_wrap(.metric~., scales = \"free\")\ncombo_plot\n\n\n\npe1 <- ggplot(\n  data = predicciones_lm_ext,\n  aes(x = yield, y = .pred)\n) +\n  geom_point(alpha = 0.3) +\n  geom_abline(slope = 1, intercept = 0, color = \"firebrick\") +\n  labs(title = \"Valor predicho vs valor real\") +\n  theme_bw()\n\npe2 <- ggplot(\n  data = predicciones_lm_ext,\n  aes(x = 1:nrow(predicciones_lm_ext), y = yield - .pred)\n) +\n  geom_point(alpha = 0.3) +\n  geom_hline(yintercept =  0, color = \"firebrick\") +\n  labs(title = \"Residuos del modelo\",\n       x = \".row\")   +\n  theme_bw()\n\npe3 <- ggplot(\n  data = predicciones_lm_ext,\n  aes(x = yield - .pred)\n) +\n  geom_density() + \n  labs(title = \"Distribución residuos del modelo\") +\n  theme_bw()\n\npe4 <- ggplot(\n  data = predicciones_lm_ext,\n  aes(sample = yield - .pred)\n) +\n  geom_qq() +\n  geom_qq_line(color = \"firebrick\") +\n  labs(title = \"Q-Q residuos del modelo\") +\n  theme_bw()\n\nggpubr::ggarrange(plotlist = list(pe1, pe2, pe3, pe4)) %>%\n  ggpubr::annotate_figure(\n    top = ggpubr::text_grob(\"Distribución residuos\", size = 15, face = \"bold\")\n  )\n\n\n\n\nLos puntos azules en los gráficos de cajas y bigotes, representan a los promedios en la fase de testeo externo. Se evidencia un cierto grado de sobreajuste en esta fase.\nLuego del testeo externo, si se desea usar el algoritmo para realizar predicciones con conjuntos de datos nuevos, se puede recalibrar nuevamente el modelo con todos los datos del entrenamiento, prueba interna y externa."
  },
  {
    "objectID": "chapter1.html#discusión",
    "href": "chapter1.html#discusión",
    "title": "2  Modelamiento en tidymodels con el motor lm.",
    "section": "2.7 Discusión",
    "text": "2.7 Discusión\nMuchos problemas predictivos pueden solucionarse con modelos de bajo costo computacional. La clave para hacer modelamiento predictivo no está en el uso de modelos “sofisticados”. Antes de comenzar con algún modelamiento, se debe realizar un adecuado proceso de ingeniería de variables, que es lo que permite el éxito del modelamiento en un 80 %. Hay que considerar que cada modelo tiene un procedimiento de preprocesamiento de los datos recomendado.\nAparentemente, según los resultados del testeo externo, este ha sido un problema en el cual no existió dependencia entre observaciones, entonces, cada una de las observaciones representaba a un individuo distinto en un único momento de tiempo y cada individuo tiene una única medición en los datos; esto permite que el modelo creado se pueda validar mediante un proceso más básico de lo que habitualmente se necesita. Pero, este este no es el caso que regularmente se presenta en las empresas agrícolas o en Crop Yield Forecasting, por ello se debe tener mucho cuidado si se intenta replicar este esquema de validación en casos en los cuales tenemos datos donde existe dependencia por individuo, por tiempo y por espacio."
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "3  Modelamiento en tidymodels con el motor ranger.",
    "section": "",
    "text": "Caso: Plant spatial traits, bee species composition, and weather conditions dataset for wild blueberry yield prediction through computer simulation modeling and machine learning algorithms (Obsie & Drummond, 2020)"
  },
  {
    "objectID": "chapter2.html#receta-de-preprocesamiento",
    "href": "chapter2.html#receta-de-preprocesamiento",
    "title": "3  Modelamiento en tidymodels con el motor ranger.",
    "section": "3.1 Receta de preprocesamiento",
    "text": "3.1 Receta de preprocesamiento\nEn este caso, evitaré transformar a la variable respuesta con la función “log”.\n\n### Recipe ----\n\ndata_rec <- recipe(formula = `yield` ~ .,\n                   x = data_train) %>%\n  step_select(where(base::is.numeric)) %>%\n  # step_nzv(all_predictors(),\n  #          -all_outcomes()) %>%\n  # step_zv(all_predictors(),\n  #          -all_outcomes()) %>%\n  step_mutate_at(where(base::is.numeric),\n                 -contains(c(\"yield\",\n                             \"RainingDays\",\n                             \"AverageRainingDays\",\n                             \"honeybee\",\n                             \"bumbles\",\n                             \"andrena\",\n                             \"osmia\",\n                             \"fruitset\",\n                             \"fruitmass\")),\n                 fn = ~log(.))\n\ndata_prep <- prep(data_rec)\ndata_train_prep <- juice(data_prep)\n# data_train_prep <- bake(data_prep, new_data = data_train)\ndata_test_prep  <- bake(data_prep, new_data = data_test)\ndata_new_prep  <- bake(data_prep, new_data = data_new)"
  },
  {
    "objectID": "chapter2.html#random-forest",
    "href": "chapter2.html#random-forest",
    "title": "3  Modelamiento en tidymodels con el motor ranger.",
    "section": "3.2 Random Forest",
    "text": "3.2 Random Forest\nEn esta sección, veremos como se entrenan, validen y testean modelos no paramétricos, tomando como ejemplo el algoritmo Random Forest. Omitiré muchas cosas del capítulo anterior por ser redundantes.\n\n3.2.1 Definir especificaciones del modelo\n\n# Inicializar un objeto de regresión lineal\nrf_model_spec <- rand_forest(\n  trees = tune(), \n  min_n = tune(), \n  mtry = tune()\n) %>% \n  # Establecer el modelo de motor\n  set_engine('ranger',                         \n             importance = \"permutation\") %>% \n  # Establecer el modo de modelo\n  set_mode('regression')\n\n\n\n3.2.2 Definir fórmula del modelo\nA diferencia del capítulo con el motor lm, el motor ranger permite usar al algoritmo Random Forest y este último puede seleccionar a las mejores variables según importancia mediante permutación.\n\nformula_rf = yield ~ .  \n\n\n\n3.2.3 Definir flujo de trabajo\n\nrf_wf <- workflow() %>%\n  add_recipe(data_rec) %>%\n  add_model(rf_model_spec,\n            formula = formula_rf) \n\ntune_args(rf_wf) #obtener todos los argumentos ajustables posibles en el flujo de trabajo\n\n# A tibble: 3 × 6\n  name  tunable id    source     component   component_id\n  <chr> <lgl>   <chr> <chr>      <chr>       <chr>       \n1 mtry  TRUE    mtry  model_spec rand_forest <NA>        \n2 trees TRUE    trees model_spec rand_forest <NA>        \n3 min_n TRUE    min_n model_spec rand_forest <NA>        \n\n\n\n\n3.2.4 Extraer el conjunto de hiperparámetros del modelo\nSimilar a lo que se realizó con la regresión Lasso. En este caso, definimos un rango de árboles que irá de 50 a 150 solo para hacer más rápida la búsqueda de hiperparámetros. Este método requiere que finalicemos la búsqueda con mtry (número de predictores)\n\nrf_params <- hardhat::extract_parameter_set_dials(rf_wf) %>% \n  update(trees = trees(range = c(50L, 150L)))\nrf_params <- rf_params %>% \n  dials::finalize(rf_params)\nrf_params\n\nCollection of 3 parameters for tuning\n\n identifier  type    object\n       mtry  mtry nparam[+]\n      trees trees nparam[+]\n      min_n min_n nparam[+]\n\n\n\n\n3.2.5 Búsqueda del mejor conjunto de Hiperparámtros mediante validación cruzada\nEn este paso se hará uso de la optimización bayesiana de hiperparámetros, el cual considero que es un proceso más sofisticado y menos costoso que la búsqueda de hiperparámetros regular o por grillas.\n\n\n\n\nset.seed(2020)\ntictoc::tic()\nrf_tune <-\n  rf_wf %>%\n  tune_bayes(\n    resamples = data_folds,\n    param_info = rf_params,\n    # por defecto es 5 (número aleatorio de combinaciones (puntos de grilla) de hiperparámetros)\n    initial = 5, \n    iter = 50,\n    metrics = multi_met, # metric_set(rmse,mae,smape),\n    control = control_bayes(\n      # El corte entero para el número de iteraciones sin mejores resultados.\n      no_improve = 10,\n      extract = identity,\n      save_pred = TRUE,\n      verbose = TRUE#,\n      # parallel_over = \"resamples\"\n    )\n  )\n\n\n\n\n❯  Generating a set of 5 initial parameter results\n\n\n✓ Initialization complete\n\n\n\n\n\n\n\n\n── Iteration 1 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=179.8 (@iter 0)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4503 candidates\n\n\ni Predicted candidates\n\n\ni mtry=6, trees=52, min_n=13\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\n♥ Newest results:   rmse=166.7 (+/-21.7)\n\n\n\n\n\n── Iteration 2 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=166.7 (@iter 1)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4467 candidates\n\n\ni Predicted candidates\n\n\ni mtry=4, trees=145, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\n♥ Newest results:   rmse=148.8 (+/-19.2)\n\n\n\n\n\n── Iteration 3 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=148.8 (@iter 2)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4469 candidates\n\n\ni Predicted candidates\n\n\ni mtry=5, trees=148, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\n♥ Newest results:   rmse=146.7 (+/-19.9)\n\n\n\n\n\n── Iteration 4 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=146.7 (@iter 3)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4443 candidates\n\n\ni Predicted candidates\n\n\ni mtry=6, trees=126, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\n♥ Newest results:   rmse=140.9 (+/-16.2)\n\n\n\n\n\n── Iteration 5 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=140.9 (@iter 4)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4478 candidates\n\n\ni Predicted candidates\n\n\ni mtry=6, trees=58, min_n=3\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=143.3 (+/-18.4)\n\n\n\n\n\n── Iteration 6 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=140.9 (@iter 4)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4483 candidates\n\n\ni Predicted candidates\n\n\ni mtry=6, trees=83, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=149.4 (+/-19.5)\n\n\n\n\n\n── Iteration 7 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=140.9 (@iter 4)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4451 candidates\n\n\ni Predicted candidates\n\n\ni mtry=6, trees=133, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=142.5 (+/-17.9)\n\n\n\n\n\n── Iteration 8 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=140.9 (@iter 4)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4491 candidates\n\n\ni Predicted candidates\n\n\ni mtry=5, trees=51, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=142.2 (+/-18.7)\n\n\n\n\n\n── Iteration 9 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=140.9 (@iter 4)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4465 candidates\n\n\ni Predicted candidates\n\n\ni mtry=6, trees=52, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\n♥ Newest results:   rmse=140.5 (+/-17.5)\n\n\n\n\n\n── Iteration 10 ────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=140.5 (@iter 9)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4472 candidates\n\n\ni Predicted candidates\n\n\ni mtry=5, trees=121, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=146.2 (+/-20.1)\n\n\n\n\n\n── Iteration 11 ────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=140.5 (@iter 9)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4492 candidates\n\n\ni Predicted candidates\n\n\ni mtry=6, trees=149, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=147 (+/-18.3)\n\n\n\n\n\n── Iteration 12 ────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=140.5 (@iter 9)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4496 candidates\n\n\ni Predicted candidates\n\n\ni mtry=4, trees=53, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=157.4 (+/-22.5)\n\n\n\n\n\n── Iteration 13 ────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=140.5 (@iter 9)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4445 candidates\n\n\ni Predicted candidates\n\n\ni mtry=6, trees=107, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=143.2 (+/-16.9)\n\n\n\n\n\n── Iteration 14 ────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=140.5 (@iter 9)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4502 candidates\n\n\ni Predicted candidates\n\n\ni mtry=6, trees=123, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=143.9 (+/-17.7)\n\n\n\n\n\n── Iteration 15 ────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=140.5 (@iter 9)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4513 candidates\n\n\ni Predicted candidates\n\n\ni mtry=6, trees=61, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=141.9 (+/-17.7)\n\n\n\n\n\n── Iteration 16 ────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=140.5 (@iter 9)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4445 candidates\n\n\ni Predicted candidates\n\n\ni mtry=5, trees=129, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=144.4 (+/-17.3)\n\n\n\n\n\n── Iteration 17 ────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=140.5 (@iter 9)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4479 candidates\n\n\ni Predicted candidates\n\n\ni mtry=6, trees=116, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=145.3 (+/-17.4)\n\n\n\n\n\n── Iteration 18 ────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=140.5 (@iter 9)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4501 candidates\n\n\ni Predicted candidates\n\n\ni mtry=6, trees=57, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=140.7 (+/-17)\n\n\n\n\n\n── Iteration 19 ────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=140.5 (@iter 9)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4452 candidates\n\n\ni Predicted candidates\n\n\ni mtry=6, trees=67, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=149.1 (+/-18.7)\n\n\n! No improvement for 10 iterations; returning current results.\n\ntictoc::toc()\n\n270.1 sec elapsed\n\n\n\n\n3.2.6 Métricas del rendimiento predictivo en fase de validación cruzada\nA partir de este paso tendremos un poco más de cuidado. Sucede que la búsqueda de hiperparámetros crea un objeto que almacena los resultados de cada conjunto de hiperparámetros contrastado en cada folio. Por ello, lo conveniente es primero tener el mejor conjunto de hiperparámetros para luego visualizar el performance predictivo de este mejor conjunto dentro de cada folio.\n\n# plot(rf_bayes_tune)\nautoplot(rf_tune)\n\n\n\nrf_tune %>%\n  collect_metrics() %>%\n  dplyr::select(-std_err) %>%\n  pivot_wider(names_from = .metric,\n              values_from = c(mean)) %>%\n  # filter(.metric == \"rmse\") %>%\n  mutate_if(base::is.numeric, round, 3) %>%\n  dplyr::arrange(rmse) %>%\n  reactable::reactable()\n\n\n\n\n\nrf_tune %>%\n  collect_metrics() %>%\n  # filter(.metric == \"rmse\") %>%\n  select(.metric, mean, std_err, min_n, mtry, trees\n  ) %>%\n  pivot_longer(min_n:mtry:trees,\n               values_to = \"value\",\n               names_to = \"parameter\") %>%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_line(show.legend = FALSE) +\n  geom_point(show.legend = FALSE) +\n  geom_errorbar(aes(ymin = mean - std_err,\n                    ymax = mean + std_err)) +\n  facet_grid(.metric~parameter, scales = \"free\") +\n  labs(x = NULL, y = \"RMSE\")\n\nWarning in x:y: numerical expression has 2 elements: only the first used\n\n\n\n\n\n\n\n3.2.7 Seleccionar el mejor conjunto de hiperparámetros\n\nbest_rf_model <- select_best(rf_tune, \"rmse\")\n\nfinal_rf <- finalize_workflow(rf_wf,\n                              best_rf_model)\nfinal_rf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_select()\n• step_mutate_at()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 6\n  trees = 52\n  min_n = 2\n\nEngine-Specific Arguments:\n  importance = permutation\n\nComputational engine: ranger \n\n\n\n\n3.2.8 Métricas del rendimiento predictivo para el mejor conjunto de hiperparámetros\nAhora rescatamos las métricas del rendimiento predictivo solo para el mejor conjunto de hiperparámetros en la fase de validación cruzada.\n\n# Métricas promedio de todas las particiones\nmetrics_rf <- \n  rf_tune %>% \n  collect_metrics(summarize = TRUE) %>%\n  dplyr::mutate(modelo = \"rf\") %>%\n  dplyr::select(-c(.estimator,.config)) %>%\n  dplyr::filter(mtry %in% best_rf_model$mtry,\n                min_n %in% best_rf_model$min_n,\n                trees %in% best_rf_model$trees)\nmetrics_rf\n\n# A tibble: 4 × 9\n   mtry trees min_n .metric    mean     n  std_err .iter modelo\n  <int> <int> <int> <chr>     <dbl> <int>    <dbl> <int> <chr> \n1     6    52     2 mae     102.       20  7.05        9 rf    \n2     6    52     2 mape      1.92     20  0.348       9 rf    \n3     6    52     2 rmse    140.       20 17.5         9 rf    \n4     6    52     2 rsq       0.989    20  0.00197     9 rf    \n\n\n\n# Métricas individuales de cada una de las particiones\nmetrics_rf_complete <- \n  rf_tune %>% \n  collect_metrics(summarize = FALSE) %>%\n  dplyr::mutate(modelo = \"rf\") %>%\n  dplyr::select(-c(.config)) %>% \n  group_by(id, mtry, min_n, trees, .metric) %>%\n  dplyr::summarise(dplyr::across(c(.estimate),\n                                 mean)) %>%\n  dplyr::filter(mtry %in% best_rf_model$mtry,\n                min_n %in% best_rf_model$min_n,\n                trees %in% best_rf_model$trees)\n\n`summarise()` has grouped output by 'id', 'mtry', 'min_n', 'trees'. You can\noverride using the `.groups` argument.\n\nmetrics_rf_complete\n\n# A tibble: 80 × 6\n# Groups:   id, mtry, min_n, trees [20]\n   id      mtry min_n trees .metric .estimate\n   <chr>  <int> <int> <int> <chr>       <dbl>\n 1 Fold01     6     2    52 mae       137.   \n 2 Fold01     6     2    52 mape        2.23 \n 3 Fold01     6     2    52 rmse      213.   \n 4 Fold01     6     2    52 rsq         0.978\n 5 Fold02     6     2    52 mae       100.   \n 6 Fold02     6     2    52 mape        1.95 \n 7 Fold02     6     2    52 rmse      150.   \n 8 Fold02     6     2    52 rsq         0.992\n 9 Fold03     6     2    52 mae        90.7  \n10 Fold03     6     2    52 mape        1.35 \n# … with 70 more rows\n\n\n\n# Valores de validación (mae y rmse) obtenidos en cada partición y repetición.\np1 <- ggplot(\n  data = metrics_rf_complete,\n  aes(x = .estimate, fill = .metric)) +\n  geom_density(alpha = 0.5) +\n  theme_bw() +\n  facet_wrap(. ~ .metric, scales = \"free\") +\n  geom_vline(\n    data = metrics_rf %>% \n    rename(\".estimate\" = \"mean\"),\n    aes(xintercept = .estimate, colour = .metric), linewidth = 1, alpha = 1,\n    linetype = 2\n  ) +\n  theme(axis.text.x=element_text(angle=90, hjust=0.5))\np2 <- ggplot(\n  data = metrics_rf_complete,\n  aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.1) +\n  geom_jitter(width = 0.05, alpha = 0.3) +\n  # coord_flip() +\n  geom_point(\n    data = metrics_rf %>% \n    rename(\".estimate\" = \"mean\"),\n    color = \"black\", size = 2, alpha = 0.5\n  ) +\n  theme_bw() +\n  facet_wrap(.metric ~ ., scales = \"free\") \n\nggpubr::ggarrange(p1, p2, ncol = 2, common.legend = TRUE, align = \"v\") %>% \n  ggpubr::annotate_figure(\n    top = ggpubr::text_grob(\"Distribución errores de validación cruzada\", size = 15)\n  )\n\n\n\n\n\nvalidation_rf_predictions <- \n  rf_tune %>% \n  tune::collect_predictions(summarize = FALSE) %>%\n  dplyr::mutate(modelo = \"rf\") %>%\n  dplyr::filter(mtry %in% best_rf_model$mtry,\n                min_n %in% best_rf_model$min_n,\n                trees %in% best_rf_model$trees)\n\np1 <- ggplot(\n  data = validation_rf_predictions,\n  aes(x = yield, y = .pred)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(slope = 1, intercept = 0, color = \"firebrick\") +\n  labs(title = \"Valor predicho vs valor real\") +\n  theme_bw()\n\np2 <- ggplot(\n  data = validation_rf_predictions,\n  aes(x = .row, y = yield - .pred)\n) +\n  geom_point(alpha = 0.3) +\n  geom_hline(yintercept =  0, color = \"firebrick\") +\n  labs(title = \"Residuos del modelo\") +\n  theme_bw()\n\np3 <- ggplot(\n  data = validation_rf_predictions,\n  aes(x = yield - .pred)\n) +\n  geom_density() + \n  labs(title = \"Distribución residuos del modelo\") +\n  theme_bw()\n\np4 <- ggplot(\n  data = validation_rf_predictions,\n  aes(sample = yield - .pred)\n) +\n  geom_qq() +\n  geom_qq_line(color = \"firebrick\") +\n  labs(title = \"Q-Q residuos del modelo\") +\n  theme_bw()\n\nggpubr::ggarrange(plotlist = list(p1, p2, p3, p4)) %>%\n  ggpubr::annotate_figure(\n    top = ggpubr::text_grob(\"Distribución residuos\", size = 15, face = \"bold\")\n  )\n\n\n\n\n\n\n3.2.9 Entrenamiento del modelo final\n\nrf_model <- final_rf %>%\n  fit(data_train) %>%\n  extract_fit_parsnip()\n\n\n\n3.2.10 Variables más importantes\nYa que hemos creado un modelo con todas las variables, es necesario revisar si existe alguna variable que aporta de forma negativa en el performance predictivo.\n\n# vip::vip(rf_model, num_features = 60)\n\nvi_rf <- \n  rf_model %>%\n  vip::vi() %>%\n  dplyr::mutate(Sign = ifelse(Importance >= 0, \"POS\", \"NEG\"),\n    Importance = abs(Importance),\n    Variable = fct_reorder(Variable, Importance),\n    Total_Importance = sum(Importance),\n    Importance = Importance/Total_Importance) %>%\n  # dplyr::filter(!Importance == 0) %>%\n  ggplot(aes(x = Importance,\n             y = Variable,\n             fill = Sign)) +\n  geom_col() +\n  geom_text(aes(label = round(Importance,4),\n                y = Variable),\n            x = 0.1) +\n  scale_fill_manual(breaks = c(\"NEG\",\"POS\"),\n                    values = c(\"red\",\"blue\")) +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(y = NULL)\nvi_rf\n\n\n\n\nSe observa que todas las variables aportan de forma positiva salvo “andrena”. En este caso, como “andrena” tiene importancia negativa se debe eliminar. Adicionalmente, se observa que las variables más imporantes fueron “fruitset”, “seeds” y “fruitmass”. Probaré repetir el proceso de entrenamiento, considerando las tres variables más importantes. El resto de variables con importancia positiva pero muy baja pueden llegar a tener importancia negativa en algún momento cuando se pruebe otros conjuntos de variables.\n\n\n3.2.11 Modificar la receta de preprocesamiento\nCon step_selec seleccionamos las tres variables más importantes.\n\n### Recipe ----\n\ndata_rec <- recipe(formula = `yield` ~ .,\n                   x = data_train) %>%\n  step_select(yield, fruitset, seeds, fruitmass) %>%\n  step_mutate_at(where(base::is.numeric),\n                 -contains(c(\"yield\",\n                             \"fruitset\",\n                             \"fruitmass\")),\n                 fn = ~log(.))\n\ndata_prep <- prep(data_rec)\ndata_train_prep <- juice(data_prep)\n# data_train_prep <- bake(data_prep, new_data = data_train)\ndata_test_prep  <- bake(data_prep, new_data = data_test)\ndata_new_prep  <- bake(data_prep, new_data = data_new)\n\n\n\n3.2.12 Redefinir flujo de trabajo\n\nrf_wf <- workflow() %>%\n  add_recipe(data_rec) %>%\n  add_model(rf_model_spec,\n            formula = formula_rf) \n\ntune_args(rf_wf) #obtener todos los argumentos ajustables posibles en el flujo de trabajo\n\n# A tibble: 3 × 6\n  name  tunable id    source     component   component_id\n  <chr> <lgl>   <chr> <chr>      <chr>       <chr>       \n1 mtry  TRUE    mtry  model_spec rand_forest <NA>        \n2 trees TRUE    trees model_spec rand_forest <NA>        \n3 min_n TRUE    min_n model_spec rand_forest <NA>        \n\n\n\n\n3.2.13 Extraer el conjunto de hiperparámetros del modelo con el mejor conjunto de variables\n\nrf_params <- hardhat::extract_parameter_set_dials(rf_wf) %>% \n  update(mtry = mtry(range = c(1L, 3L)))\nrf_params <- rf_params %>% \n  dials::finalize(rf_params)\nrf_params\n\nCollection of 3 parameters for tuning\n\n identifier  type    object\n       mtry  mtry nparam[+]\n      trees trees nparam[+]\n      min_n min_n nparam[+]\n\n\n\n\n3.2.14 Repetir búsqueda del mejor conjunto de Hiperparámtros mediante validación cruzada\n\nset.seed(2020)\ntictoc::tic()\nrf_tune <-\n  rf_wf %>%\n  tune_bayes(\n    resamples = data_folds,\n    param_info = rf_params,\n    # por defecto es 5 (número aleatorio de combinaciones (puntos de grilla) de hiperparámetros)\n    initial = 5, \n    iter = 50,\n    metrics = multi_met, # metric_set(rmse,mae,smape),\n    control = control_bayes(\n      # El corte entero para el número de iteraciones sin mejores resultados.\n      no_improve = 10,\n      extract = identity,\n      save_pred = TRUE,\n      verbose = TRUE#,\n      # parallel_over = \"resamples\"\n    )\n  )\n\n\n\n\n❯  Generating a set of 5 initial parameter results\n\n\n✓ Initialization complete\n\n\n\n\n\n\n\n\n── Iteration 1 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=156.8 (@iter 0)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4962 candidates\n\n\ni Predicted candidates\n\n\ni mtry=2, trees=68, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\n♥ Newest results:   rmse=154.2 (+/-12.6)\n\n\n\n\n\n── Iteration 2 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=154.2 (@iter 1)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4967 candidates\n\n\ni Predicted candidates\n\n\ni mtry=3, trees=1977, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=163.2 (+/-15.7)\n\n\n\n\n\n── Iteration 3 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=154.2 (@iter 1)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4968 candidates\n\n\ni Predicted candidates\n\n\ni mtry=2, trees=1991, min_n=3\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=156.2 (+/-13.7)\n\n\n\n\n\n── Iteration 4 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=154.2 (@iter 1)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4960 candidates\n\n\ni Predicted candidates\n\n\ni mtry=2, trees=18, min_n=3\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=162.8 (+/-14.7)\n\n\n\n\n\n── Iteration 5 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=154.2 (@iter 1)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4955 candidates\n\n\ni Predicted candidates\n\n\ni mtry=1, trees=1028, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=171.9 (+/-14.4)\n\n\n\n\n\n── Iteration 6 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=154.2 (@iter 1)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4957 candidates\n\n\ni Predicted candidates\n\n\ni mtry=1, trees=431, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=173.5 (+/-14.7)\n\n\n\n\n\n── Iteration 7 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=154.2 (@iter 1)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4956 candidates\n\n\ni Predicted candidates\n\n\ni mtry=2, trees=1976, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=155.1 (+/-13.1)\n\n\n\n\n\n── Iteration 8 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=154.2 (@iter 1)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4963 candidates\n\n\ni Predicted candidates\n\n\ni mtry=2, trees=951, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=156 (+/-13.6)\n\n\n\n\n\n── Iteration 9 ─────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=154.2 (@iter 1)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4962 candidates\n\n\ni Predicted candidates\n\n\ni mtry=3, trees=31, min_n=2\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=165.5 (+/-16.1)\n\n\n\n\n\n── Iteration 10 ────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=154.2 (@iter 1)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4957 candidates\n\n\ni Predicted candidates\n\n\ni mtry=3, trees=1967, min_n=8\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=168.6 (+/-17.8)\n\n\n\n\n\n── Iteration 11 ────────────────────────────────────────────────────────────────\n\n\n\n\n\ni Current best:     rmse=154.2 (@iter 1)\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 4970 candidates\n\n\ni Predicted candidates\n\n\ni mtry=1, trees=1908, min_n=22\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\nⓧ Newest results:   rmse=224.3 (+/-18.2)\n\n\n! No improvement for 10 iterations; returning current results.\n\ntictoc::toc()\n\n212.86 sec elapsed\n\n\n\n\n3.2.15 Métricas del rendimiento predictivo en fase de validación cruzada con el mejor conjunto de variables\n\n# plot(rf_bayes_tune)\nautoplot(rf_tune)\n\n\n\nrf_tune %>%\n  collect_metrics() %>%\n  dplyr::select(-std_err) %>%\n  pivot_wider(names_from = .metric,\n              values_from = c(mean)) %>%\n  # filter(.metric == \"rmse\") %>%\n  mutate_if(base::is.numeric, round, 3) %>%\n  dplyr::arrange(rmse) %>%\n  reactable::reactable()\n\n\n\n\n\nrf_tune %>%\n  collect_metrics() %>%\n  # filter(.metric == \"rmse\") %>%\n  select(.metric, mean, std_err, min_n, mtry, trees\n  ) %>%\n  pivot_longer(min_n:mtry:trees,\n               values_to = \"value\",\n               names_to = \"parameter\") %>%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_line(show.legend = FALSE) +\n  geom_point(show.legend = FALSE) +\n  geom_errorbar(aes(ymin = mean - std_err,\n                    ymax = mean + std_err)) +\n  facet_grid(.metric~parameter, scales = \"free\") +\n  labs(x = NULL, y = \"RMSE\")\n\nWarning in x:y: numerical expression has 2 elements: only the first used\n\n\n\n\n\n\n\n3.2.16 Seleccionar el mejor conjunto de hiperparámetros con el mejor conjunto de variables\n\nbest_rf_model <- select_best(rf_tune, \"rmse\")\n\nfinal_rf <- finalize_workflow(rf_wf,\n                              best_rf_model)\nfinal_rf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_select()\n• step_mutate_at()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 2\n  trees = 68\n  min_n = 2\n\nEngine-Specific Arguments:\n  importance = permutation\n\nComputational engine: ranger \n\n\n\n\n3.2.17 Métricas del rendimiento predictivo para el mejor conjunto de hiperparámetros con el mejor conjunto de variables\nAhora rescatamos las métricas del rendimiento predictivo solo para el mejor conjunto de hiperparámetros en la fase de validación cruzada.\n\n# Métricas promedio de todas las particiones\nmetrics_rf <- \n  rf_tune %>% \n  collect_metrics(summarize = TRUE) %>%\n  dplyr::mutate(modelo = \"rf\") %>%\n  dplyr::select(-c(.estimator,.config)) %>%\n  dplyr::filter(mtry %in% best_rf_model$mtry,\n                min_n %in% best_rf_model$min_n,\n                trees %in% best_rf_model$trees)\nmetrics_rf\n\n# A tibble: 4 × 9\n   mtry trees min_n .metric    mean     n  std_err .iter modelo\n  <int> <int> <int> <chr>     <dbl> <int>    <dbl> <int> <chr> \n1     2    68     2 mae     114.       20  6.62        1 rf    \n2     2    68     2 mape      2.04     20  0.273       1 rf    \n3     2    68     2 rmse    154.       20 12.6         1 rf    \n4     2    68     2 rsq       0.986    20  0.00164     1 rf    \n\n\n\n# Métricas individuales de cada una de las particiones\nmetrics_rf_complete <- \n  rf_tune %>% \n  collect_metrics(summarize = FALSE) %>%\n  dplyr::mutate(modelo = \"rf\") %>%\n  dplyr::select(-c(.config)) %>% \n  group_by(id, mtry, min_n, trees, .metric) %>%\n  dplyr::summarise(dplyr::across(c(.estimate),\n                                 mean)) %>%\n  dplyr::filter(mtry %in% best_rf_model$mtry,\n                min_n %in% best_rf_model$min_n,\n                trees %in% best_rf_model$trees)\n\n`summarise()` has grouped output by 'id', 'mtry', 'min_n', 'trees'. You can\noverride using the `.groups` argument.\n\nmetrics_rf_complete\n\n# A tibble: 80 × 6\n# Groups:   id, mtry, min_n, trees [20]\n   id      mtry min_n trees .metric .estimate\n   <chr>  <int> <int> <int> <chr>       <dbl>\n 1 Fold01     2     2    68 mae       126.   \n 2 Fold01     2     2    68 mape        2.18 \n 3 Fold01     2     2    68 rmse      140.   \n 4 Fold01     2     2    68 rsq         0.991\n 5 Fold02     2     2    68 mae       111.   \n 6 Fold02     2     2    68 mape        2.12 \n 7 Fold02     2     2    68 rmse      181.   \n 8 Fold02     2     2    68 rsq         0.984\n 9 Fold03     2     2    68 mae       124.   \n10 Fold03     2     2    68 mape        1.83 \n# … with 70 more rows\n\n\n\n# Valores de validación (mae y rmse) obtenidos en cada partición y repetición.\np1 <- ggplot(\n  data = metrics_rf_complete,\n  aes(x = .estimate, fill = .metric)) +\n  geom_density(alpha = 0.5) +\n  theme_bw() +\n  facet_wrap(. ~ .metric, scales = \"free\") +\n  geom_vline(\n    data = metrics_rf %>% \n    rename(\".estimate\" = \"mean\"),\n    aes(xintercept = .estimate, colour = .metric), linewidth = 1, alpha = 1,\n    linetype = 2\n  ) +\n  theme(axis.text.x=element_text(angle=90, hjust=0.5))\np2 <- ggplot(\n  data = metrics_rf_complete,\n  aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.1) +\n  geom_jitter(width = 0.05, alpha = 0.3) +\n  # coord_flip() +\n  geom_point(\n    data = metrics_rf %>% \n    rename(\".estimate\" = \"mean\"),\n    color = \"black\", size = 2, alpha = 0.5\n  ) +\n  theme_bw() +\n  facet_wrap(.metric ~ ., scales = \"free\") \n\nggpubr::ggarrange(p1, p2, ncol = 2, common.legend = TRUE, align = \"v\") %>% \n  ggpubr::annotate_figure(\n    top = ggpubr::text_grob(\"Distribución errores de validación cruzada\", size = 15)\n  )\n\n\n\n\n\nvalidation_rf_predictions <- \n  rf_tune %>% \n  tune::collect_predictions(summarize = FALSE) %>%\n  dplyr::mutate(modelo = \"rf\") %>%\n  dplyr::filter(mtry %in% best_rf_model$mtry,\n                min_n %in% best_rf_model$min_n,\n                trees %in% best_rf_model$trees)\n\np1 <- ggplot(\n  data = validation_rf_predictions,\n  aes(x = yield, y = .pred)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(slope = 1, intercept = 0, color = \"firebrick\") +\n  labs(title = \"Valor predicho vs valor real\") +\n  theme_bw()\n\np2 <- ggplot(\n  data = validation_rf_predictions,\n  aes(x = .row, y = yield - .pred)\n) +\n  geom_point(alpha = 0.3) +\n  geom_hline(yintercept =  0, color = \"firebrick\") +\n  labs(title = \"Residuos del modelo\") +\n  theme_bw()\n\np3 <- ggplot(\n  data = validation_rf_predictions,\n  aes(x = yield - .pred)\n) +\n  geom_density() + \n  labs(title = \"Distribución residuos del modelo\") +\n  theme_bw()\n\np4 <- ggplot(\n  data = validation_rf_predictions,\n  aes(sample = yield - .pred)\n) +\n  geom_qq() +\n  geom_qq_line(color = \"firebrick\") +\n  labs(title = \"Q-Q residuos del modelo\") +\n  theme_bw()\n\nggpubr::ggarrange(plotlist = list(p1, p2, p3, p4)) %>%\n  ggpubr::annotate_figure(\n    top = ggpubr::text_grob(\"Distribución residuos\", size = 15, face = \"bold\")\n  )\n\n\n\n\n\n\n3.2.18 Entrenamiento del modelo final con el mejor conjunto de variables\n\nrf_model <- final_rf %>%\n  fit(data_train) %>%\n  extract_fit_parsnip()\n\n\n\n3.2.19 Verificación de la importancia de variables\nYa que hemos creado un modelo con todas las variables, es necesario revisar si existe alguna variable que aporta de forma negativa en el performance predictivo.\n\n# vip::vip(rf_model, num_features = 60)\n\nvi_rf <- \n  rf_model %>%\n  vip::vi() %>%\n  dplyr::mutate(Sign = ifelse(Importance >= 0, \"POS\", \"NEG\"),\n    Importance = abs(Importance),\n    Variable = fct_reorder(Variable, Importance),\n    Total_Importance = sum(Importance),\n    Importance = Importance/Total_Importance) %>%\n  # dplyr::filter(!Importance == 0) %>%\n  ggplot(aes(x = Importance,\n             y = Variable,\n             fill = Sign)) +\n  geom_col() +\n  geom_text(aes(label = round(Importance,4),\n                y = Variable),\n            x = 0.1) +\n  scale_fill_manual(breaks = c(\"NEG\",\"POS\"),\n                    values = c(\"red\",\"blue\")) +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(y = NULL)\nvi_rf\n\n\n\n\n\n3.2.19.1 Rescatando las especififaciones del modelo para futuras recalibraciones\n\nspec <- rf_model %>% extract_spec_parsnip()\n\n\n\n\n3.2.20 Testeo interno\nAhora, testeamos el modelo final con los datos de prueba (data_test).\n\n#### Testeo interno ----\n\n# PREDICCIÓN TEST ----\n# =============================================================================\npredicciones <- rf_model %>%\n  predict(\n    new_data = data_test_prep,\n    type = \"numeric\"\n  )\n\n# MÉTRICAS TEST ----\n# =============================================================================\npredicciones_rf <- predicciones %>% \n  bind_cols(data_test_prep %>% dplyr::select(yield)) %>%\n  dplyr::mutate(modelo = \"rf\")\n\nsummary(predicciones_rf)\n\n     .pred          yield         modelo         \n Min.   :3494   Min.   :3049   Length:127        \n 1st Qu.:5832   1st Qu.:5792   Class :character  \n Median :6756   Median :6796   Mode  :character  \n Mean   :6629   Mean   :6616                     \n 3rd Qu.:7458   3rd Qu.:7434                     \n Max.   :8691   Max.   :8562                     \n\n# Error de test\n\nerror_test_rf  <- multi_met(\n  data     = predicciones_rf,\n  truth    = yield,\n  estimate = .pred,\n  na_rm    = TRUE\n) %>%\n  dplyr::mutate(\n    modelo = \"rf\"\n  )\nerror_test_rf\n\n# A tibble: 4 × 4\n  .metric .estimator .estimate modelo\n  <chr>   <chr>          <dbl> <chr> \n1 rmse    standard     137.    rf    \n2 rsq     standard       0.987 rf    \n3 mape    standard       1.76  rf    \n4 mae     standard     108.    rf    \n\ncombo_plot <- metrics_rf_complete %>% \n  ggplot(aes(x = .metric, y = .estimate)) +\n  geom_jitter(width = 0.2) +\n  geom_boxplot(width = 0.3, alpha = 0.5) +\n  geom_point(\n    data = metrics_rf %>% rename(\".estimate\" = \"mean\"),\n    color = \"green\", size = 2, alpha = 0.5\n  ) +\n  geom_point(\n    data = error_test_rf,\n    color = \"red\", size = 2\n  ) +\n  facet_wrap(.metric~., scales = \"free\")\ncombo_plot\n\n\n\npe1 <- ggplot(\n  data = predicciones_rf,\n  aes(x = yield, y = .pred)\n) +\n  geom_point(alpha = 0.3) +\n  geom_abline(slope = 1, intercept = 0, color = \"firebrick\") +\n  labs(title = \"Valor predicho vs valor real\") +\n  theme_bw()\n\npe2 <- ggplot(\n  data = predicciones_rf,\n  aes(x = 1:nrow(predicciones_rf), y = yield - .pred)\n) +\n  geom_point(alpha = 0.3) +\n  geom_hline(yintercept =  0, color = \"firebrick\") +\n  labs(title = \"Residuos del modelo\",\n       x = \".row\")   +\n  theme_bw()\n\npe3 <- ggplot(\n  data = predicciones_rf,\n  aes(x = yield - .pred)\n) +\n  geom_density() + \n  labs(title = \"Distribución residuos del modelo\") +\n  theme_bw()\n\npe4 <- ggplot(\n  data = predicciones_rf,\n  aes(sample = yield - .pred)\n) +\n  geom_qq() +\n  geom_qq_line(color = \"firebrick\") +\n  labs(title = \"Q-Q residuos del modelo\") +\n  theme_bw()\n\nggpubr::ggarrange(plotlist = list(pe1, pe2, pe3, pe4)) %>%\n  ggpubr::annotate_figure(\n    top = ggpubr::text_grob(\"Distribución residuos\", size = 15, face = \"bold\")\n  )\n\n\n\n\nSe puede observar que no hay evidencia de sobreajuste.\n\n\n3.2.21 Recalibración inicial del modelo\n\nrf_model_rec <- rf_model %>%\n  extract_spec_parsnip() %>%\n  fit(\n    formula_rf,\n    data = bind_rows(data_train_prep,\n                     data_test_prep))\n\n\n\n3.2.22 Testeo externo\nFinalmente, el testeo externo con el conjunto de datos para testeo externo (data_new).\n\n#### Testeo externo ----\n\n# PREDICCIÓN TEST ----\n# =============================================================================\npredicciones_ext <- rf_model_rec %>%\n  predict(\n    new_data = data_new_prep,\n    type = \"numeric\"\n  )\n\n# MÉTRICAS TEST ----\n# =============================================================================\npredicciones_rf_ext <- predicciones_ext %>% \n  bind_cols(data_new_prep %>% select(yield)) %>%\n  dplyr::mutate(modelo = \"rf\")\n\nsummary(predicciones_rf_ext)\n\n     .pred          yield         modelo         \n Min.   :2564   Min.   :1946   Length:354        \n 1st Qu.:4436   1st Qu.:4514   Class :character  \n Median :5326   Median :5555   Mode  :character  \n Mean   :5201   Mean   :5360                     \n 3rd Qu.:5919   3rd Qu.:6237                     \n Max.   :7263   Max.   :7805                     \n\n# Error de test\n\nerror_test_rf_ext  <- multi_met(\n  data     = predicciones_rf_ext,\n  truth    = yield,\n  estimate = .pred,\n  na_rm    = TRUE\n) %>%\n  dplyr::mutate(\n    modelo = \"rf\"\n  )\nerror_test_rf_ext\n\n# A tibble: 4 × 4\n  .metric .estimator .estimate modelo\n  <chr>   <chr>          <dbl> <chr> \n1 rmse    standard     316.    rf    \n2 rsq     standard       0.957 rf    \n3 mape    standard       4.92  rf    \n4 mae     standard     254.    rf    \n\ncombo_plot <- metrics_rf_complete %>%\n  ggplot(aes(x = .metric, y = .estimate)) +\n  geom_jitter(width = 0.2) +\n  geom_boxplot(width = 0.3, alpha = 0.5) +\n  geom_point(\n    data = metrics_rf %>% rename(\".estimate\" = \"mean\"),\n    color = \"green\", size = 2, alpha = 0.5\n  ) +\n  geom_point(\n    data = error_test_rf,\n    color = \"red\", size = 2, alpha = 0.5\n  ) +\n  geom_point(\n    data = error_test_rf_ext,\n    color = \"blue\", size = 2, alpha = 0.5\n  ) +\n  facet_wrap(.metric~., scales = \"free\")\ncombo_plot\n\n\n\npe1 <- ggplot(\n  data = predicciones_rf_ext,\n  aes(x = yield, y = .pred)\n) +\n  geom_point(alpha = 0.3) +\n  geom_abline(slope = 1, intercept = 0, color = \"firebrick\") +\n  labs(title = \"Valor predicho vs valor real\") +\n  theme_bw()\n\npe2 <- ggplot(\n  data = predicciones_rf_ext,\n  aes(x = 1:nrow(predicciones_rf_ext), y = yield - .pred)\n) +\n  geom_point(alpha = 0.3) +\n  geom_hline(yintercept =  0, color = \"firebrick\") +\n  labs(title = \"Residuos del modelo\",\n       x = \".row\")   +\n  theme_bw()\n\npe3 <- ggplot(\n  data = predicciones_rf_ext,\n  aes(x = yield - .pred)\n) +\n  geom_density() + \n  labs(title = \"Distribución residuos del modelo\") +\n  theme_bw()\n\npe4 <- ggplot(\n  data = predicciones_rf_ext,\n  aes(sample = yield - .pred)\n) +\n  geom_qq() +\n  geom_qq_line(color = \"firebrick\") +\n  labs(title = \"Q-Q residuos del modelo\") +\n  theme_bw()\n\nggpubr::ggarrange(plotlist = list(pe1, pe2, pe3, pe4)) %>%\n  ggpubr::annotate_figure(\n    top = ggpubr::text_grob(\"Distribución residuos\", size = 15, face = \"bold\")\n  )\n\n\n\n\nCon el nuevo conjunto de variables, se puede observar que el rendimiento predictivo del modelo tiene una variancia un poco mayor en comparación al modelo lineal general."
  },
  {
    "objectID": "chapter2.html#discusión",
    "href": "chapter2.html#discusión",
    "title": "3  Modelamiento en tidymodels con el motor ranger.",
    "section": "3.3 Discusión",
    "text": "3.3 Discusión\nUna de las cosas que discuto regularmente es el uso de modelos no paramétricos en Crop Yield Forecast. Sucede que estos modelos requieren de mayor cantidad de datos a comparación de los modelos paramétricos y adicionalmente tienden a tener un mayor riesgo de sobreajuste en problemas de regresión. Por ello, los modelos no paramétricos son más recomendados en problemas de clasificación y cuando se tiene una gran cantidad de datos.\nEn mi corta experiencia, he notado que tener una gran cantidad de datos históricos en empresas de agroexportación es muy difícil, debido a que regularmente se instalan nuevas variadades de cultivos o por otro lado, las empresas deciden producir nuevos productos agrícolas debido al crecimiento de su demanda y/o de su precio.\nEs por este problema que mi recomendación es usar modelos paramétricos cuando se inicia un proyecto de Crop Yield Forecast, hasta que se tenga la suficiente información histórica para cambiar los procesos con algoritmos no paramétricos.\nPor otro lado, los modelos paramétricos son menos costosos computacionalmente a comparación de los no paramétricos, por ello, son muy útiles para experimentar con nuevas técnicas de preprocesamiento, procesos de validación de los modelos o aplicar alguna innovación."
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "4  Revisión de muchos modelos y Ensamblado de modelos en tidymodels.",
    "section": "",
    "text": "Caso: Plant spatial traits, bee species composition, and weather conditions dataset for wild blueberry yield prediction through computer simulation modeling and machine learning algorithms (Obsie & Drummond, 2020)"
  },
  {
    "objectID": "chapter3.html#modelamiento-del-rendimiento-del-arándano-silvestre.",
    "href": "chapter3.html#modelamiento-del-rendimiento-del-arándano-silvestre.",
    "title": "4  Revisión de muchos modelos y Ensamblado de modelos en tidymodels.",
    "section": "4.1 Modelamiento del rendimiento del arándano silvestre.",
    "text": "4.1 Modelamiento del rendimiento del arándano silvestre.\nEn este caso, solo trabajaré con las tres variables más importantes que se han hallado en el Modelamiento en tidymodels con el motor ranger.\n\n4.1.1 Recetas de preprocesamiento\nPara evitar errores, las recetas de preprocesamiento no deben de incluir pasos de selección de variables.\n\n### Recipe ----\n\ndata_rec <- recipe(formula = `yield` ~ .,\n                   x = data_train) %>%\n  step_mutate_at(where(base::is.numeric),\n                 -contains(c(\"yield\",\n                             \"fruitset\",\n                             \"fruitmass\")),\n                 fn = ~log(.))\n\nnormalized_rec <- \n   data_rec %>% \n   step_normalize(all_predictors()) \n\npoly_recipe <- \n   normalized_rec %>% \n   step_poly(all_predictors()) %>% \n   step_interact(~ all_predictors():all_predictors())\n\n\n\n4.1.2 Especificación de los modelos\nEn este paso, especificamos todos los modelos a probar.\n\nlinear_reg_spec <- \n   linear_reg(penalty = tune(), mixture = tune()) %>% \n   set_engine(\"glmnet\")\n\nnnet_spec <- \n   mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>% \n   set_engine(\"nnet\", MaxNWts = 2600) %>% \n   set_mode(\"regression\")\n\nmars_spec <- \n   mars(prod_degree = tune()) %>%  #<- use GCV to choose terms\n   set_engine(\"earth\") %>% \n   set_mode(\"regression\")\n\nsvm_r_spec <- \n   svm_rbf(cost = tune(), rbf_sigma = tune()) %>% \n   set_engine(\"kernlab\") %>% \n   set_mode(\"regression\")\n\nsvm_p_spec <- \n   svm_poly(cost = tune(), degree = tune()) %>% \n   set_engine(\"kernlab\") %>% \n   set_mode(\"regression\")\n\nknn_spec <- \n   nearest_neighbor(neighbors = tune(), dist_power = tune(), weight_func = tune()) %>% \n   set_engine(\"kknn\") %>% \n   set_mode(\"regression\")\n\ncart_spec <- \n   decision_tree(cost_complexity = tune(), min_n = tune()) %>% \n   set_engine(\"rpart\") %>% \n   set_mode(\"regression\")\n\nbag_cart_spec <- \n   bag_tree() %>% \n   set_engine(\"rpart\", times = 50L) %>% \n   set_mode(\"regression\")\n\nrf_spec <- \n   rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% \n   set_engine(\"ranger\") %>% \n   set_mode(\"regression\")\n\nxgb_spec <- \n   boost_tree(tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(), \n              min_n = tune(), sample_size = tune(), trees = tune()) %>% \n   set_engine(\"xgboost\") %>% \n   set_mode(\"regression\")\n\ncubist_spec <- \n   cubist_rules(committees = tune(), neighbors = tune()) %>% \n   set_engine(\"Cubist\")\n\n\n\n4.1.3 Extraer el conjunto de hiperparámetros de la red neuronal monocapa\nEn el caso de la red neuronal monocapa, fue necesario crear una especificación del hiperparámetro del número de unidades ocultas.\n\nnnet_param <- \n   nnet_spec %>% \n   extract_parameter_set_dials() %>% \n   update(hidden_units = hidden_units(range = c(1L, 27L)))\n\n\n\n4.1.4 Creación del conjunto de flujos de trabajo\nEn este caso, es necesario especificar qué receta de preprocesamiento se aplica a cada modelo. Puede aplicarse más de una receta por modelo, considerando que cada interacción de receta y modelo creará un algoritmo único.\n\nnormalized <- \n   workflow_set(\n      preproc = list(normalized = normalized_rec), \n      models = list(SVM_radial = svm_r_spec, SVM_poly = svm_p_spec, \n                    KNN = knn_spec, neural_network = nnet_spec)\n   )\nnormalized\n\n# A workflow set/tibble: 4 × 4\n  wflow_id                  info             option    result    \n  <chr>                     <list>           <list>    <list>    \n1 normalized_SVM_radial     <tibble [1 × 4]> <opts[0]> <list [0]>\n2 normalized_SVM_poly       <tibble [1 × 4]> <opts[0]> <list [0]>\n3 normalized_KNN            <tibble [1 × 4]> <opts[0]> <list [0]>\n4 normalized_neural_network <tibble [1 × 4]> <opts[0]> <list [0]>\n\n\n\nnormalized %>% extract_workflow(id = \"normalized_KNN\")\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_mutate_at()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (regression)\n\nMain Arguments:\n  neighbors = tune()\n  weight_func = tune()\n  dist_power = tune()\n\nComputational engine: kknn \n\n\n\nnormalized <- \n   normalized %>% \n   option_add(param_info = nnet_param, id = \"normalized_neural_network\")\nnormalized\n\n# A workflow set/tibble: 4 × 4\n  wflow_id                  info             option    result    \n  <chr>                     <list>           <list>    <list>    \n1 normalized_SVM_radial     <tibble [1 × 4]> <opts[0]> <list [0]>\n2 normalized_SVM_poly       <tibble [1 × 4]> <opts[0]> <list [0]>\n3 normalized_KNN            <tibble [1 × 4]> <opts[0]> <list [0]>\n4 normalized_neural_network <tibble [1 × 4]> <opts[1]> <list [0]>\n\n\n\nsimple_pre_proc <- \n   workflow_set(\n      preproc = list(simple = data_rec), \n      models = list(MARS = mars_spec, CART = cart_spec, CART_bagged = bag_cart_spec,\n                    RF = rf_spec, boosting = xgb_spec, Cubist = cubist_spec)\n   )\nsimple_pre_proc\n\n# A workflow set/tibble: 6 × 4\n  wflow_id           info             option    result    \n  <chr>              <list>           <list>    <list>    \n1 simple_MARS        <tibble [1 × 4]> <opts[0]> <list [0]>\n2 simple_CART        <tibble [1 × 4]> <opts[0]> <list [0]>\n3 simple_CART_bagged <tibble [1 × 4]> <opts[0]> <list [0]>\n4 simple_RF          <tibble [1 × 4]> <opts[0]> <list [0]>\n5 simple_boosting    <tibble [1 × 4]> <opts[0]> <list [0]>\n6 simple_Cubist      <tibble [1 × 4]> <opts[0]> <list [0]>\n\n\n\nwith_features <- \n   workflow_set(\n      preproc = list(full_quad = poly_recipe), \n      models = list(linear_reg = linear_reg_spec, KNN = knn_spec)\n   )\n\n\nall_workflows <- \n   bind_rows(simple_pre_proc, normalized, with_features) %>% \n   # Make the workflow ID's a little more simple: \n   mutate(wflow_id = gsub(\"(simple_)|(normalized_)\", \"\", wflow_id))\nall_workflows\n\n# A workflow set/tibble: 12 × 4\n   wflow_id             info             option    result    \n   <chr>                <list>           <list>    <list>    \n 1 MARS                 <tibble [1 × 4]> <opts[0]> <list [0]>\n 2 CART                 <tibble [1 × 4]> <opts[0]> <list [0]>\n 3 CART_bagged          <tibble [1 × 4]> <opts[0]> <list [0]>\n 4 RF                   <tibble [1 × 4]> <opts[0]> <list [0]>\n 5 boosting             <tibble [1 × 4]> <opts[0]> <list [0]>\n 6 Cubist               <tibble [1 × 4]> <opts[0]> <list [0]>\n 7 SVM_radial           <tibble [1 × 4]> <opts[0]> <list [0]>\n 8 SVM_poly             <tibble [1 × 4]> <opts[0]> <list [0]>\n 9 KNN                  <tibble [1 × 4]> <opts[0]> <list [0]>\n10 neural_network       <tibble [1 × 4]> <opts[1]> <list [0]>\n11 full_quad_linear_reg <tibble [1 × 4]> <opts[0]> <list [0]>\n12 full_quad_KNN        <tibble [1 × 4]> <opts[0]> <list [0]>\n\n\n\n\n4.1.5 Ajuste y evaluación de los modelos\nPrimero, probaremos con la calibración bayesiana de hiperparámetros. En aquellos algoritmos donde no es necesario calibrar hiperparámetros, se prueba un ajuste por defecto en cada folio.\n\n\n\n\nbayes_ctrl <-\n   control_bayes(\n      # El corte entero para el número de iteraciones sin mejores resultados.\n      no_improve = 10,\n      extract = identity,\n      save_pred = TRUE,\n      verbose = TRUE,\n      parallel_over = \"everything\"\n    )\n\nfull_results_time <- \n   system.time(\n      bayes_results <- \n         all_workflows %>% \n         workflow_map(seed = 1503,\n                      resamples = data_folds,\n                      initial = 5, \n                      iter = 50,\n                      metrics = multi_met,\n                      control = bayes_ctrl,\n                      verbose = TRUE)\n   )\n\ni  1 of 12 tuning:     MARS\n\n\nWarning: The `...` are not used in this function but one or more objects were\npassed: 'initial', 'iter'\n\n\n✔  1 of 12 tuning:     MARS (23.9s)\n\n\ni  2 of 12 tuning:     CART\n\n\nWarning: The `...` are not used in this function but one or more objects were\npassed: 'initial', 'iter'\n\n\n✔  2 of 12 tuning:     CART (47s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni  3 of 12 resampling: CART_bagged\n\n\n✔  3 of 12 resampling: CART_bagged (1m 6s)\n\n\ni  4 of 12 tuning:     RF\n\n\nWarning: The `...` are not used in this function but one or more objects were\npassed: 'initial', 'iter'\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔  4 of 12 tuning:     RF (1m 8.1s)\n\n\ni  5 of 12 tuning:     boosting\n\n\nWarning: The `...` are not used in this function but one or more objects were\npassed: 'initial', 'iter'\n\n\n✔  5 of 12 tuning:     boosting (1m 28.5s)\n\n\ni  6 of 12 tuning:     Cubist\n\n\nWarning: The `...` are not used in this function but one or more objects were\npassed: 'initial', 'iter'\n\n\n✔  6 of 12 tuning:     Cubist (1m 3s)\n\n\ni  7 of 12 tuning:     SVM_radial\n\n\nWarning: The `...` are not used in this function but one or more objects were\npassed: 'initial', 'iter'\n\n\n✔  7 of 12 tuning:     SVM_radial (35.5s)\n\n\ni  8 of 12 tuning:     SVM_poly\n\n\nWarning: The `...` are not used in this function but one or more objects were\npassed: 'initial', 'iter'\n\n\n✔  8 of 12 tuning:     SVM_poly (18.5s)\n\n\ni  9 of 12 tuning:     KNN\n\n\nWarning: The `...` are not used in this function but one or more objects were\npassed: 'initial', 'iter'\n\n\n✔  9 of 12 tuning:     KNN (17.8s)\n\n\ni 10 of 12 tuning:     neural_network\n\n\nWarning: The `...` are not used in this function but one or more objects were\npassed: 'initial', 'iter'\n\n\n✔ 10 of 12 tuning:     neural_network (21.4s)\n\n\ni 11 of 12 tuning:     full_quad_linear_reg\n\n\nWarning: The `...` are not used in this function but one or more objects were\npassed: 'initial', 'iter'\n\n\n✔ 11 of 12 tuning:     full_quad_linear_reg (20.2s)\n\n\ni 12 of 12 tuning:     full_quad_KNN\n\n\nWarning: The `...` are not used in this function but one or more objects were\npassed: 'initial', 'iter'\n\n\n✔ 12 of 12 tuning:     full_quad_KNN (23.3s)\n\n\n\nbayes_results\n\n# A workflow set/tibble: 12 × 4\n   wflow_id             info             option    result   \n   <chr>                <list>           <list>    <list>   \n 1 MARS                 <tibble [1 × 4]> <opts[5]> <tune[+]>\n 2 CART                 <tibble [1 × 4]> <opts[5]> <tune[+]>\n 3 CART_bagged          <tibble [1 × 4]> <opts[5]> <rsmp[+]>\n 4 RF                   <tibble [1 × 4]> <opts[5]> <tune[+]>\n 5 boosting             <tibble [1 × 4]> <opts[5]> <tune[+]>\n 6 Cubist               <tibble [1 × 4]> <opts[5]> <tune[+]>\n 7 SVM_radial           <tibble [1 × 4]> <opts[5]> <tune[+]>\n 8 SVM_poly             <tibble [1 × 4]> <opts[5]> <tune[+]>\n 9 KNN                  <tibble [1 × 4]> <opts[5]> <tune[+]>\n10 neural_network       <tibble [1 × 4]> <opts[6]> <tune[+]>\n11 full_quad_linear_reg <tibble [1 × 4]> <opts[5]> <tune[+]>\n12 full_quad_KNN        <tibble [1 × 4]> <opts[5]> <tune[+]>\n\n\nEl siguiente resultado rankea u ordena los resultados de los algoritmos en validación cruzada según el rmse.\n\nbayes_results %>% \n   rank_results() %>% \n   filter(.metric == \"rmse\") %>% \n   select(model, .config, rmse = mean, rank)\n\n# A tibble: 103 × 4\n   model        .config                rmse  rank\n   <chr>        <chr>                 <dbl> <int>\n 1 linear_reg   Preprocessor1_Model08  110.     1\n 2 linear_reg   Preprocessor1_Model04  110.     2\n 3 linear_reg   Preprocessor1_Model05  111.     3\n 4 linear_reg   Preprocessor1_Model03  111.     4\n 5 linear_reg   Preprocessor1_Model06  111.     5\n 6 cubist_rules Preprocessor1_Model06  111.     6\n 7 mlp          Preprocessor1_Model01  111.     7\n 8 cubist_rules Preprocessor1_Model02  111.     8\n 9 cubist_rules Preprocessor1_Model04  111.     9\n10 linear_reg   Preprocessor1_Model10  112.    10\n# … with 93 more rows\n\n\nSe puede imprimir los resultados globales a continuación.\n\nautoplot(\n   bayes_results,\n   rank_metric = \"rmse\",  # <- how to order models\n   metric = \"rmse\",       # <- which metric to visualize\n   select_best = TRUE     # <- one point per workflow\n) +\n   geom_text(aes(y = mean - 40, label = wflow_id), angle = 90, hjust = 1) +\n   lims(y = c(0, 250)) +\n   theme(legend.position = \"none\")\n\n\n\n\nO por cada algoritmo probado, especificándolo.\n\nautoplot(bayes_results, id = \"Cubist\", metric = \"rmse\")\n\n\n\n\n\n\n4.1.6 Selección eficaz de los modelos\nPara stacking es recomendable usar la búsqueda eficiente de cuadrículas mediante carreras con modelos ANOVA.\n\nrace_ctrl <-\n   control_race(\n      save_pred = TRUE,\n      parallel_over = \"everything\",\n      save_workflow = TRUE,\n      verbose = TRUE\n   )\n\nfull_race_results_time <- \n   system.time(\n      race_results <-\n   all_workflows %>%\n   workflow_map(\n      \"tune_race_anova\",\n      seed = 1503,\n      resamples = data_folds,\n      grid = 25,\n      control = race_ctrl,\n      verbose = TRUE,\n      metrics = multi_met)\n   )\n\ni  1 of 12 tuning:     MARS\n\n\n✔  1 of 12 tuning:     MARS (12.9s)\n\n\ni  2 of 12 tuning:     CART\n\n\n✔  2 of 12 tuning:     CART (26.5s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni  3 of 12 resampling: CART_bagged\n\n\n✔  3 of 12 resampling: CART_bagged (21s)\n\n\ni  4 of 12 tuning:     RF\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔  4 of 12 tuning:     RF (42.5s)\n\n\ni  5 of 12 tuning:     boosting\n\n\n✔  5 of 12 tuning:     boosting (54.2s)\n\n\ni  6 of 12 tuning:     Cubist\n\n\n✔  6 of 12 tuning:     Cubist (55.3s)\n\n\ni  7 of 12 tuning:     SVM_radial\n\n\n✔  7 of 12 tuning:     SVM_radial (21.6s)\n\n\ni  8 of 12 tuning:     SVM_poly\n\n\n✔  8 of 12 tuning:     SVM_poly (38.5s)\n\n\ni  9 of 12 tuning:     KNN\n\n\n✔  9 of 12 tuning:     KNN (51.2s)\n\n\ni 10 of 12 tuning:     neural_network\n\n\n✔ 10 of 12 tuning:     neural_network (57.8s)\n\n\ni 11 of 12 tuning:     full_quad_linear_reg\n\n\n✔ 11 of 12 tuning:     full_quad_linear_reg (55.2s)\n\n\ni 12 of 12 tuning:     full_quad_KNN\n\n\n✔ 12 of 12 tuning:     full_quad_KNN (52.1s)\n\n\n\nrace_results\n\n# A workflow set/tibble: 12 × 4\n   wflow_id             info             option    result   \n   <chr>                <list>           <list>    <list>   \n 1 MARS                 <tibble [1 × 4]> <opts[4]> <race[+]>\n 2 CART                 <tibble [1 × 4]> <opts[4]> <race[+]>\n 3 CART_bagged          <tibble [1 × 4]> <opts[4]> <rsmp[+]>\n 4 RF                   <tibble [1 × 4]> <opts[4]> <race[+]>\n 5 boosting             <tibble [1 × 4]> <opts[4]> <race[+]>\n 6 Cubist               <tibble [1 × 4]> <opts[4]> <race[+]>\n 7 SVM_radial           <tibble [1 × 4]> <opts[4]> <race[+]>\n 8 SVM_poly             <tibble [1 × 4]> <opts[4]> <race[+]>\n 9 KNN                  <tibble [1 × 4]> <opts[4]> <race[+]>\n10 neural_network       <tibble [1 × 4]> <opts[5]> <race[+]>\n11 full_quad_linear_reg <tibble [1 × 4]> <opts[4]> <race[+]>\n12 full_quad_KNN        <tibble [1 × 4]> <opts[4]> <race[+]>\n\n\n\nautoplot(\n   race_results,\n   rank_metric = \"rmse\",  \n   metric = \"rmse\",       \n   select_best = TRUE    \n) +\n   geom_text(aes(y = mean - 40, label = wflow_id), angle = 90, hjust = 1) +\n   lims(y = c(0, 260)) +\n   theme(legend.position = \"none\")\n\n\n\n\n\nres_ranks <-\n  race_results %>%\n  rank_results('rmse') %>%\n  filter(.metric == 'rmse') %>%\n  select(wflow_id, model, .config, rmse = mean, rank) %>%\n  group_by(model) %>%\n  slice_min(rank, with_ties = FALSE) %>%\n  ungroup() %>%\n  arrange(rank)\n\nres_ranks\n\n# A tibble: 11 × 5\n   wflow_id             model            .config                rmse  rank\n   <chr>                <chr>            <chr>                 <dbl> <int>\n 1 full_quad_linear_reg linear_reg       Preprocessor1_Model09  109.     1\n 2 Cubist               cubist_rules     Preprocessor1_Model07  110.     5\n 3 MARS                 mars             Preprocessor1_Model1   117.    25\n 4 SVM_poly             svm_poly         Preprocessor1_Model09  127.    26\n 5 neural_network       mlp              Preprocessor1_Model19  128.    27\n 6 SVM_radial           svm_rbf          Preprocessor1_Model13  140.    40\n 7 boosting             boost_tree       Preprocessor1_Model03  154.    45\n 8 full_quad_KNN        nearest_neighbor Preprocessor1_Model02  154.    46\n 9 RF                   rand_forest      Preprocessor1_Model08  155.    47\n10 CART_bagged          bag_tree         Preprocessor1_Model1   165.    54\n11 CART                 decision_tree    Preprocessor1_Model02  213.    66\n\n\nA continuación se muestra un plot de calibración entre ambos procesos de validación cruzada.\n\nmatched_results <- \n   workflowsets::rank_results(race_results, select_best = TRUE) %>% \n   dplyr::select(wflow_id, .metric, race = mean, config_race = .config) %>% \n   dplyr::inner_join(\n      workflowsets::rank_results(bayes_results, select_best = TRUE) %>% \n         dplyr::select(wflow_id, .metric, complete = mean, \n                config_complete = .config, model),\n      by = c(\"wflow_id\", \".metric\"),\n   ) %>%  \n   dplyr::filter(.metric == \"rmse\")\n\nmatched_results %>% \n   ggplot(aes(x = complete, y = race)) + \n   geom_abline(lty = 3) + \n   geom_point() + \n   geom_text_repel(aes(label = model)) +\n   coord_obs_pred() + \n   labs(x = \"Complete Bayes RMSE\", y = \"Racing RMSE\")\n\n\n\n\nSe elige el mejor modelo a continuación:\n\nwflow_id_best <- \n  res_ranks %>% \n  slice_min(rank, with_ties = FALSE) %>% \n  pull(wflow_id)\n\nwf_best <-\n  race_results %>% \n  extract_workflow_set_result(wflow_id_best) %>% \n  select_best(metric = 'rmse')\n\n\n\n4.1.7 Finalizar un modelo\n\n\n\n\n\n\nRealizamos el ajuste final del mejor modelo.\n\nfit_best <-\n  race_results %>% \n  pull_workflow(wflow_id_best) %>% \n  finalize_workflow(wf_best) %>% \n  last_fit(split = data_split)\n\nWarning: `pull_workflow()` was deprecated in workflowsets 0.1.0.\nℹ Please use `extract_workflow()` instead.\n\nmetrics_best <-\n  fit_best %>% \n  collect_metrics()\n\nfit_best %>% \n  collect_predictions()\n\n# A tibble: 127 × 5\n   id               .pred  .row yield .config             \n   <chr>            <dbl> <int> <dbl> <chr>               \n 1 train/test split 6041.     1 5996. Preprocessor1_Model1\n 2 train/test split 5499.     3 5502. Preprocessor1_Model1\n 3 train/test split 4148.     6 3968. Preprocessor1_Model1\n 4 train/test split 3262.     8 3139. Preprocessor1_Model1\n 5 train/test split 6345.     9 6126. Preprocessor1_Model1\n 6 train/test split 6256.    12 6244. Preprocessor1_Model1\n 7 train/test split 5668.    15 5482. Preprocessor1_Model1\n 8 train/test split 6617.    17 6578. Preprocessor1_Model1\n 9 train/test split 7328.    18 7267. Preprocessor1_Model1\n10 train/test split 5883.    19 5747. Preprocessor1_Model1\n# … with 117 more rows\n\n\nObservamos el ajuste a nivel de testeo interno.\n\nfit_best %>% \n   collect_predictions() %>% \n   ggplot(aes(x = yield, y = .pred)) + \n   geom_abline(color = \"gray50\", lty = 2) + \n   geom_point(alpha = 0.5) + \n   coord_obs_pred() + \n   labs(x = \"observed\", y = \"predicted\")\n\n\n\n\nReajustamos el modelo.\n\nmodel_test_results_ex <- race_results %>% \n  pull_workflow(wflow_id_best) %>% \n  finalize_workflow(wf_best) %>% \n  fit(data = bind_rows(data_train,\n                     data_test))\n\nY comprobamos las métricas a nivel de testeo externo.\n\n# PREDICCIÓN TEST ----\n# =============================================================================\npredicciones_ext <- model_test_results_ex %>%\n  predict(\n    new_data = data_new,\n    # poly_recipe %>% \n    #   prep() %>%\n    #   bake(new_data = data_new),\n    type = \"numeric\"\n  )\n\n# MÉTRICAS TEST ----\n# =============================================================================\npredicciones_bm_ext <- predicciones_ext %>% \n  bind_cols(data_new %>% select(yield)) %>%\n  dplyr::mutate(modelo = \"best model\")\n\nsummary(predicciones_bm_ext)\n\n     .pred          yield         modelo         \n Min.   :2297   Min.   :1946   Length:354        \n 1st Qu.:4510   1st Qu.:4514   Class :character  \n Median :5299   Median :5555   Mode  :character  \n Mean   :5177   Mean   :5360                     \n 3rd Qu.:5896   3rd Qu.:6237                     \n Max.   :7036   Max.   :7805                     \n\n# Error de test\n\nerror_test_bm_ext  <- multi_met(\n  data     = predicciones_bm_ext,\n  truth    = yield,\n  estimate = .pred,\n  na_rm    = TRUE\n) %>%\n  dplyr::mutate(\n    modelo = \"best model\"\n  )\nerror_test_bm_ext\n\n# A tibble: 4 × 4\n  .metric .estimator .estimate modelo    \n  <chr>   <chr>          <dbl> <chr>     \n1 rmse    standard     320.    best model\n2 rsq     standard       0.975 best model\n3 mape    standard       4.83  best model\n4 mae     standard     263.    best model\n\n\n\npredicciones_bm_ext %>% \n   ggplot(aes(x = yield, y = .pred)) + \n   geom_abline(color = \"gray50\", lty = 2) + \n   geom_point(alpha = 0.5) + \n   coord_obs_pred() + \n   labs(x = \"observed\", y = \"predicted\")\n\n\n\n\n\n\n4.1.8 Creación del conjunto de entrenamiento para el apilamiento\nEn este paso, añadiremos todos los modelos a un proceso de apilamiento o stacking.\n\ndata_stack <- \n  stacks() %>% \n  add_candidates(race_results)\n\nWarning: The inputted `candidates` argument `boosting` generated notes during\ntuning/resampling. Model stacking may fail due to these issues; see\n`collect_notes()` (`?tune::collect_notes()`) if so.\n\n\nWarning: The inputted `candidates` argument `neural_network` generated notes during\ntuning/resampling. Model stacking may fail due to these issues; see\n`collect_notes()` (`?tune::collect_notes()`) if so.\n\ndata_stack\n\n# A data stack with 12 model definitions and 68 candidate members:\n#   MARS: 1 model configuration\n#   CART: 3 model configurations\n#   CART_bagged: 1 model configuration\n#   RF: 2 model configurations\n#   boosting: 2 model configurations\n#   Cubist: 13 model configurations\n#   SVM_radial: 1 model configuration\n#   SVM_poly: 8 model configurations\n#   KNN: 14 model configurations\n#   neural_network: 10 model configurations\n#   full_quad_linear_reg: 11 model configurations\n#   full_quad_KNN: 2 model configurations\n# Outcome: yield (numeric)\n\n\n\n\n4.1.9 Mezclar las predicciones\nA continuación verificamos las métricas del modelo apilado en con una regresión Lasso.\n\nset.seed(2002)\nens <- blend_predictions(data_stack, \n                         penalty = 10^seq(-2, -0.5, length = 20),\n                         metric = multi_met)\n\nVerificamos la evolución de las métricas a medida que cambia el lambda.\n\nautoplot(ens)\n\n\n\n\nPodemos validar los pesos de cada modelo.\n\nens\n\n── A stacked ensemble model ─────────────────────────────────────\n\nOut of 68 possible candidate members, the ensemble retained 9.\nPenalty: 0.316227766016838.\nMixture: 1.\n\nThe 9 highest weighted members are:\n\n\n# A tibble: 9 × 3\n  member                    type         weight\n  <chr>                     <chr>         <dbl>\n1 full_quad_linear_reg_1_09 linear_reg   0.245 \n2 Cubist_1_10               cubist_rules 0.226 \n3 Cubist_1_07               cubist_rules 0.154 \n4 MARS_1_1                  mars         0.0975\n5 Cubist_1_06               cubist_rules 0.0883\n6 neural_network_1_17       mlp          0.0714\n7 neural_network_1_14       mlp          0.0467\n8 Cubist_1_01               cubist_rules 0.0423\n9 boosting_1_04             boost_tree   0.0300\n\n\n\nMembers have not yet been fitted with `fit_members()`.\n\n\n\nautoplot(ens, \"weights\") +\n  geom_text(aes(x = weight + 0.01, label = model), hjust = 0) + \n  theme(legend.position = \"none\") +\n  lims(x = c(-0.01, 0.3))\n\n\n\n\n\n\n4.1.10 Ajustar los modelos de los miembros\nTodos aquellos modelos cuyo peso o coeficiente sea diferente de cero, serán considerados miembros. Los miembros deben entrenarse en el conjunto de entrenamiento completo.\n\nens_fit <- fit_members(ens)\n\nSi deseas, puede verificar el ajuste de cada miembro. Por ejemplo, del mejor modelo.\n\ncollect_parameters(ens_fit, \"full_quad_linear_reg\")\n\n# A tibble: 11 × 4\n   member                     penalty mixture  coef\n   <chr>                        <dbl>   <dbl> <dbl>\n 1 full_quad_linear_reg_1_09 8.56e- 2   0.384 0.245\n 2 full_quad_linear_reg_1_11 1.98e- 1   0.447 0    \n 3 full_quad_linear_reg_1_12 2.07e- 2   0.496 0    \n 4 full_quad_linear_reg_1_13 8.13e-10   0.518 0    \n 5 full_quad_linear_reg_1_14 1.96e- 7   0.562 0    \n 6 full_quad_linear_reg_1_15 2.05e- 4   0.585 0    \n 7 full_quad_linear_reg_1_16 3.13e- 5   0.633 0    \n 8 full_quad_linear_reg_1_17 4.71e- 2   0.692 0    \n 9 full_quad_linear_reg_1_18 6.19e- 1   0.732 0    \n10 full_quad_linear_reg_1_19 4.73e- 5   0.767 0    \n11 full_quad_linear_reg_1_20 4.03e-10   0.779 0    \n\n\n\n\n4.1.11 Resultados del conjunto de pruebas\nPodemos realizar el testeo interno.\n\nens_test_pred <- \n  predict(ens_fit, data_test) %>% \n  bind_cols(data_test)\n\nens_test_pred %>% \n  multi_met(yield, .pred)\n\n# A tibble: 4 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard     105.   \n2 rsq     standard       0.992\n3 mape    standard       1.37 \n4 mae     standard      85.2  \n\nggplot(ens_test_pred) +\n  aes(x = yield, \n      y = .pred) +\n  geom_abline(color = \"gray50\", lty = 2) + \n  geom_point() + \n  coord_obs_pred()\n\n\n\n\nEn este paso, verificamos como cambian las métricas de cada modelo en el testeo interno. Es necesario observar si el modelo apilado a mejorado su rendimiento predictivo con respecto al mejor modelo.\n\nmember_preds <- \n  data_test %>%\n  select(yield) %>%\n  bind_cols(predict(ens_fit, data_test, members = TRUE))\n\nmap_dfr(member_preds, .f = rmse, truth = yield, data = member_preds) %>%\n  mutate(member = colnames(member_preds))\n\n# A tibble: 11 × 4\n   .metric .estimator .estimate member                   \n   <chr>   <chr>          <dbl> <chr>                    \n 1 rmse    standard          0  yield                    \n 2 rmse    standard        105. .pred                    \n 3 rmse    standard        121. MARS_1_1                 \n 4 rmse    standard        130. boosting_1_04            \n 5 rmse    standard        109. Cubist_1_01              \n 6 rmse    standard        108. Cubist_1_06              \n 7 rmse    standard        108. Cubist_1_07              \n 8 rmse    standard        110. Cubist_1_10              \n 9 rmse    standard        110. neural_network_1_17      \n10 rmse    standard        113. neural_network_1_14      \n11 rmse    standard        106. full_quad_linear_reg_1_09\n\nmap_dfr(member_preds, .f = mape, truth = yield, data = member_preds) %>%\n  mutate(member = colnames(member_preds))\n\n# A tibble: 11 × 4\n   .metric .estimator .estimate member                   \n   <chr>   <chr>          <dbl> <chr>                    \n 1 mape    standard        0    yield                    \n 2 mape    standard        1.37 .pred                    \n 3 mape    standard        1.57 MARS_1_1                 \n 4 mape    standard        1.60 boosting_1_04            \n 5 mape    standard        1.40 Cubist_1_01              \n 6 mape    standard        1.41 Cubist_1_06              \n 7 mape    standard        1.42 Cubist_1_07              \n 8 mape    standard        1.43 Cubist_1_10              \n 9 mape    standard        1.41 neural_network_1_17      \n10 mape    standard        1.43 neural_network_1_14      \n11 mape    standard        1.39 full_quad_linear_reg_1_09\n\nmap_dfr(member_preds, .f = mae, truth = yield, data = member_preds) %>%\n  mutate(member = colnames(member_preds))\n\n# A tibble: 11 × 4\n   .metric .estimator .estimate member                   \n   <chr>   <chr>          <dbl> <chr>                    \n 1 mae     standard         0   yield                    \n 2 mae     standard        85.2 .pred                    \n 3 mae     standard        96.0 MARS_1_1                 \n 4 mae     standard       102.  boosting_1_04            \n 5 mae     standard        88.7 Cubist_1_01              \n 6 mae     standard        87.6 Cubist_1_06              \n 7 mae     standard        88.0 Cubist_1_07              \n 8 mae     standard        89.9 Cubist_1_10              \n 9 mae     standard        87.8 neural_network_1_17      \n10 mae     standard        88.5 neural_network_1_14      \n11 mae     standard        85.5 full_quad_linear_reg_1_09\n\nmap_dfr(member_preds, .f = rsq, truth = yield, data = member_preds) %>%\n  mutate(member = colnames(member_preds))\n\n# A tibble: 11 × 4\n   .metric .estimator .estimate member                   \n   <chr>   <chr>          <dbl> <chr>                    \n 1 rsq     standard       1     yield                    \n 2 rsq     standard       0.992 .pred                    \n 3 rsq     standard       0.990 MARS_1_1                 \n 4 rsq     standard       0.988 boosting_1_04            \n 5 rsq     standard       0.992 Cubist_1_01              \n 6 rsq     standard       0.992 Cubist_1_06              \n 7 rsq     standard       0.992 Cubist_1_07              \n 8 rsq     standard       0.991 Cubist_1_10              \n 9 rsq     standard       0.992 neural_network_1_17      \n10 rsq     standard       0.991 neural_network_1_14      \n11 rsq     standard       0.992 full_quad_linear_reg_1_09\n\n\nAhora el testeo externo.\n\nens_test_pred_ex <- \n  predict(ens_fit, data_new) %>% \n  bind_cols(data_new)\n\nens_test_pred_ex %>% \n  multi_met(yield, .pred)\n\n# A tibble: 4 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard     316.   \n2 rsq     standard       0.973\n3 mape    standard       4.91 \n4 mae     standard     261.   \n\nggplot(ens_test_pred_ex) +\n  aes(x = yield, \n      y = .pred) +\n  geom_abline(color = \"gray50\", lty = 2) + \n  geom_point() + \n  coord_obs_pred()"
  }
]